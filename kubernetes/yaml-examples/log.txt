* 
* ==> Audit <==
* |--------------|------|----------|-----------|---------|---------------------|---------------------|
|   Command    | Args | Profile  |   User    | Version |     Start Time      |      End Time       |
|--------------|------|----------|-----------|---------|---------------------|---------------------|
| start        |      | minikube | mitrandir | v1.32.0 | 23 Feb 24 22:33 CST | 23 Feb 24 22:36 CST |
| dashboard    |      | minikube | mitrandir | v1.32.0 | 23 Feb 24 22:36 CST |                     |
| start        |      | minikube | mitrandir | v1.32.0 | 24 Feb 24 01:45 CST | 24 Feb 24 01:46 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 24 Feb 24 23:33 CST | 24 Feb 24 23:33 CST |
| start        |      | minikube | mitrandir | v1.32.0 | 24 Feb 24 23:36 CST | 24 Feb 24 23:38 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 25 Feb 24 16:04 CST | 25 Feb 24 16:04 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 25 Feb 24 19:18 CST | 25 Feb 24 19:18 CST |
| start        |      | minikube | mitrandir | v1.32.0 | 25 Feb 24 19:23 CST | 25 Feb 24 19:23 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 26 Feb 24 00:32 CST | 26 Feb 24 00:32 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 26 Feb 24 19:07 CST | 26 Feb 24 19:07 CST |
| start        |      | minikube | mitrandir | v1.32.0 | 26 Feb 24 19:25 CST | 26 Feb 24 19:26 CST |
| start        |      | minikube | mitrandir | v1.32.0 | 26 Feb 24 22:51 CST | 26 Feb 24 22:51 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 26 Feb 24 23:51 CST | 26 Feb 24 23:51 CST |
| update-check |      | minikube | mitrandir | v1.32.0 | 28 Feb 24 01:21 CST | 28 Feb 24 01:21 CST |
| start        |      | minikube | mitrandir | v1.32.0 | 28 Feb 24 01:24 CST | 28 Feb 24 01:24 CST |
| start        |      | minikube | mitrandir | v1.32.0 | 28 Feb 24 16:14 CST | 28 Feb 24 16:15 CST |
|--------------|------|----------|-----------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/02/28 16:14:35
Running on machine: ubuntu-rogx
Binary: Built with gc go1.21.3 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0228 16:14:35.280074   21825 out.go:296] Setting OutFile to fd 1 ...
I0228 16:14:35.280427   21825 out.go:348] isatty.IsTerminal(1) = true
I0228 16:14:35.280432   21825 out.go:309] Setting ErrFile to fd 2...
I0228 16:14:35.280436   21825 out.go:348] isatty.IsTerminal(2) = true
I0228 16:14:35.280647   21825 root.go:338] Updating PATH: /home/mitrandir/.minikube/bin
W0228 16:14:35.280761   21825 root.go:314] Error reading config file at /home/mitrandir/.minikube/config/config.json: open /home/mitrandir/.minikube/config/config.json: no such file or directory
I0228 16:14:35.282377   21825 out.go:303] Setting JSON to false
I0228 16:14:35.284495   21825 start.go:128] hostinfo: {"hostname":"ubuntu-rogx","uptime":2583,"bootTime":1709155893,"procs":355,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.5.0-21-generic","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"15c2036c-b076-43f4-aa5d-aeff6ce5ece4"}
I0228 16:14:35.284561   21825 start.go:138] virtualization:  
I0228 16:14:35.285801   21825 out.go:177] üòÑ  minikube v1.32.0 on Ubuntu 22.04
I0228 16:14:35.289609   21825 notify.go:220] Checking for updates...
I0228 16:14:35.292044   21825 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0228 16:14:35.292180   21825 driver.go:378] Setting default libvirt URI to qemu:///system
I0228 16:14:35.338708   21825 docker.go:122] docker version: linux-25.0.3:Docker Engine - Community
I0228 16:14:35.338860   21825 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0228 16:14:35.719750   21825 info.go:266] docker info: {ID:f6c9e70c-44f5-4c2d-977c-b91580595cb2 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:44 SystemTime:2024-02-28 16:14:35.704052988 -0600 CST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-21-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8280236032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ubuntu-rogx Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6]] Warnings:<nil>}}
I0228 16:14:35.719839   21825 docker.go:295] overlay module found
I0228 16:14:35.721512   21825 out.go:177] ‚ú®  Using the docker driver based on existing profile
I0228 16:14:35.722869   21825 start.go:298] selected driver: docker
I0228 16:14:35.722879   21825 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mitrandir:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0228 16:14:35.722946   21825 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0228 16:14:35.723040   21825 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0228 16:14:35.842045   21825 info.go:266] docker info: {ID:f6c9e70c-44f5-4c2d-977c-b91580595cb2 Containers:2 ContainersRunning:0 ContainersPaused:0 ContainersStopped:2 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:44 SystemTime:2024-02-28 16:14:35.822507229 -0600 CST LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.5.0-21-generic OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8280236032 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:ubuntu-rogx Labels:[] ExperimentalBuild:false ServerVersion:25.0.3 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:ae07eda36dd25f8a1b98dfbf587313b99c0190bb Expected:ae07eda36dd25f8a1b98dfbf587313b99c0190bb} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.24.6]] Warnings:<nil>}}
I0228 16:14:35.842829   21825 cni.go:84] Creating CNI manager for ""
I0228 16:14:35.842848   21825 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0228 16:14:35.842860   21825 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mitrandir:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0228 16:14:35.845642   21825 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0228 16:14:35.847164   21825 cache.go:121] Beginning downloading kic base image for docker with docker
I0228 16:14:35.848373   21825 out.go:177] üöú  Pulling base image ...
I0228 16:14:35.850008   21825 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0228 16:14:35.850324   21825 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0228 16:14:35.850380   21825 preload.go:148] Found local preload: /home/mitrandir/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I0228 16:14:35.850388   21825 cache.go:56] Caching tarball of preloaded images
I0228 16:14:35.850616   21825 preload.go:174] Found /home/mitrandir/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0228 16:14:35.850626   21825 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0228 16:14:35.850768   21825 profile.go:148] Saving config to /home/mitrandir/.minikube/profiles/minikube/config.json ...
I0228 16:14:35.889345   21825 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0228 16:14:35.889358   21825 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0228 16:14:35.889395   21825 cache.go:194] Successfully downloaded all kic artifacts
I0228 16:14:35.889431   21825 start.go:365] acquiring machines lock for minikube: {Name:mk38003383bf1d275bf33dc24a3d6fa50245fe53 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0228 16:14:35.889677   21825 start.go:369] acquired machines lock for "minikube" in 158.707¬µs
I0228 16:14:35.889698   21825 start.go:96] Skipping create...Using existing machine configuration
I0228 16:14:35.889710   21825 fix.go:54] fixHost starting: 
I0228 16:14:35.889955   21825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0228 16:14:35.934187   21825 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0228 16:14:35.934205   21825 fix.go:128] unexpected machine state, will restart: <nil>
I0228 16:14:35.935877   21825 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I0228 16:14:35.936891   21825 cli_runner.go:164] Run: docker start minikube
I0228 16:14:37.029455   21825 cli_runner.go:217] Completed: docker start minikube: (1.092536017s)
I0228 16:14:37.029550   21825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0228 16:14:37.060587   21825 kic.go:430] container "minikube" state is running.
I0228 16:14:37.061189   21825 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0228 16:14:37.094439   21825 profile.go:148] Saving config to /home/mitrandir/.minikube/profiles/minikube/config.json ...
I0228 16:14:37.094959   21825 machine.go:88] provisioning docker machine ...
I0228 16:14:37.095001   21825 ubuntu.go:169] provisioning hostname "minikube"
I0228 16:14:37.095052   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:37.121633   21825 main.go:141] libmachine: Using SSH client type: native
I0228 16:14:37.123106   21825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0228 16:14:37.123118   21825 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0228 16:14:37.124227   21825 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0228 16:14:40.308444   21825 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0228 16:14:40.308513   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:40.334366   21825 main.go:141] libmachine: Using SSH client type: native
I0228 16:14:40.335031   21825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0228 16:14:40.335044   21825 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0228 16:14:40.468312   21825 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0228 16:14:40.468339   21825 ubuntu.go:175] set auth options {CertDir:/home/mitrandir/.minikube CaCertPath:/home/mitrandir/.minikube/certs/ca.pem CaPrivateKeyPath:/home/mitrandir/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/mitrandir/.minikube/machines/server.pem ServerKeyPath:/home/mitrandir/.minikube/machines/server-key.pem ClientKeyPath:/home/mitrandir/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/mitrandir/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/mitrandir/.minikube}
I0228 16:14:40.468622   21825 ubuntu.go:177] setting up certificates
I0228 16:14:40.468631   21825 provision.go:83] configureAuth start
I0228 16:14:40.468693   21825 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0228 16:14:40.495078   21825 provision.go:138] copyHostCerts
I0228 16:14:40.496578   21825 exec_runner.go:144] found /home/mitrandir/.minikube/ca.pem, removing ...
I0228 16:14:40.496604   21825 exec_runner.go:203] rm: /home/mitrandir/.minikube/ca.pem
I0228 16:14:40.496657   21825 exec_runner.go:151] cp: /home/mitrandir/.minikube/certs/ca.pem --> /home/mitrandir/.minikube/ca.pem (1086 bytes)
I0228 16:14:40.497796   21825 exec_runner.go:144] found /home/mitrandir/.minikube/cert.pem, removing ...
I0228 16:14:40.497809   21825 exec_runner.go:203] rm: /home/mitrandir/.minikube/cert.pem
I0228 16:14:40.497860   21825 exec_runner.go:151] cp: /home/mitrandir/.minikube/certs/cert.pem --> /home/mitrandir/.minikube/cert.pem (1127 bytes)
I0228 16:14:40.499041   21825 exec_runner.go:144] found /home/mitrandir/.minikube/key.pem, removing ...
I0228 16:14:40.499053   21825 exec_runner.go:203] rm: /home/mitrandir/.minikube/key.pem
I0228 16:14:40.499107   21825 exec_runner.go:151] cp: /home/mitrandir/.minikube/certs/key.pem --> /home/mitrandir/.minikube/key.pem (1675 bytes)
I0228 16:14:40.499814   21825 provision.go:112] generating server cert: /home/mitrandir/.minikube/machines/server.pem ca-key=/home/mitrandir/.minikube/certs/ca.pem private-key=/home/mitrandir/.minikube/certs/ca-key.pem org=mitrandir.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0228 16:14:40.681029   21825 provision.go:172] copyRemoteCerts
I0228 16:14:40.681238   21825 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0228 16:14:40.681276   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:40.707763   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:14:40.809189   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/machines/server.pem --> /etc/docker/server.pem (1208 bytes)
I0228 16:14:40.857853   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0228 16:14:40.902003   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0228 16:14:40.953921   21825 provision.go:86] duration metric: configureAuth took 485.268075ms
I0228 16:14:40.953938   21825 ubuntu.go:193] setting minikube options for container-runtime
I0228 16:14:40.954643   21825 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0228 16:14:40.954721   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:40.980654   21825 main.go:141] libmachine: Using SSH client type: native
I0228 16:14:40.981595   21825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0228 16:14:40.981606   21825 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0228 16:14:41.113919   21825 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0228 16:14:41.113933   21825 ubuntu.go:71] root file system type: overlay
I0228 16:14:41.114019   21825 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0228 16:14:41.114078   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:41.138376   21825 main.go:141] libmachine: Using SSH client type: native
I0228 16:14:41.138656   21825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0228 16:14:41.138721   21825 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0228 16:14:41.293147   21825 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0228 16:14:41.293218   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:41.317867   21825 main.go:141] libmachine: Using SSH client type: native
I0228 16:14:41.318207   21825 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x808a40] 0x80b720 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0228 16:14:41.318222   21825 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0228 16:14:41.459082   21825 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0228 16:14:41.459096   21825 machine.go:91] provisioned docker machine in 4.364126639s
I0228 16:14:41.459103   21825 start.go:300] post-start starting for "minikube" (driver="docker")
I0228 16:14:41.459111   21825 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0228 16:14:41.459258   21825 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0228 16:14:41.459310   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:41.487361   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:14:41.593893   21825 ssh_runner.go:195] Run: cat /etc/os-release
I0228 16:14:41.601118   21825 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0228 16:14:41.601144   21825 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0228 16:14:41.601152   21825 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0228 16:14:41.601162   21825 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0228 16:14:41.601171   21825 filesync.go:126] Scanning /home/mitrandir/.minikube/addons for local assets ...
I0228 16:14:41.602002   21825 filesync.go:126] Scanning /home/mitrandir/.minikube/files for local assets ...
I0228 16:14:41.602799   21825 start.go:303] post-start completed in 143.68664ms
I0228 16:14:41.602877   21825 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0228 16:14:41.602930   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:41.626869   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:14:41.723464   21825 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0228 16:14:41.733453   21825 fix.go:56] fixHost completed within 5.843744186s
I0228 16:14:41.733467   21825 start.go:83] releasing machines lock for "minikube", held for 5.843781617s
I0228 16:14:41.733533   21825 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0228 16:14:41.761625   21825 ssh_runner.go:195] Run: cat /version.json
I0228 16:14:41.761660   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:41.761988   21825 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0228 16:14:41.762035   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:14:41.786391   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:14:41.790313   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:14:42.517048   21825 ssh_runner.go:195] Run: systemctl --version
I0228 16:14:42.532281   21825 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0228 16:14:42.542528   21825 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0228 16:14:42.581081   21825 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0228 16:14:42.581166   21825 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0228 16:14:42.600085   21825 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0228 16:14:42.600107   21825 start.go:472] detecting cgroup driver to use...
I0228 16:14:42.600136   21825 detect.go:199] detected "systemd" cgroup driver on host os
I0228 16:14:42.600244   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0228 16:14:42.630716   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0228 16:14:42.652791   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0228 16:14:42.673339   21825 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I0228 16:14:42.673399   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0228 16:14:42.694592   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0228 16:14:42.713051   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0228 16:14:42.732360   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0228 16:14:42.749273   21825 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0228 16:14:42.769892   21825 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0228 16:14:42.787318   21825 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0228 16:14:42.802988   21825 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0228 16:14:42.820147   21825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0228 16:14:42.928046   21825 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0228 16:14:43.062192   21825 start.go:472] detecting cgroup driver to use...
I0228 16:14:43.062299   21825 detect.go:199] detected "systemd" cgroup driver on host os
I0228 16:14:43.062358   21825 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0228 16:14:43.085732   21825 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0228 16:14:43.085839   21825 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0228 16:14:43.110288   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0228 16:14:43.145195   21825 ssh_runner.go:195] Run: which cri-dockerd
I0228 16:14:43.153658   21825 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0228 16:14:43.175880   21825 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0228 16:14:43.212537   21825 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0228 16:14:43.373562   21825 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0228 16:14:43.496302   21825 docker.go:560] configuring docker to use "systemd" as cgroup driver...
I0228 16:14:43.496398   21825 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I0228 16:14:43.529523   21825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0228 16:14:43.653727   21825 ssh_runner.go:195] Run: sudo systemctl restart docker
I0228 16:14:45.040416   21825 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.386665683s)
I0228 16:14:45.040481   21825 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0228 16:14:45.151419   21825 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0228 16:14:45.264802   21825 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0228 16:14:45.382480   21825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0228 16:14:45.508360   21825 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0228 16:14:45.547465   21825 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0228 16:14:45.666781   21825 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0228 16:14:46.129943   21825 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0228 16:14:46.130035   21825 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0228 16:14:46.141925   21825 start.go:540] Will wait 60s for crictl version
I0228 16:14:46.142064   21825 ssh_runner.go:195] Run: which crictl
I0228 16:14:46.150311   21825 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0228 16:14:46.461047   21825 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0228 16:14:46.461108   21825 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0228 16:14:46.621457   21825 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0228 16:14:46.675933   21825 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0228 16:14:46.676118   21825 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0228 16:14:46.714648   21825 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0228 16:14:46.728394   21825 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0228 16:14:46.757291   21825 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0228 16:14:46.757352   21825 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0228 16:14:46.809511   21825 docker.go:671] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0228 16:14:46.809527   21825 docker.go:601] Images already preloaded, skipping extraction
I0228 16:14:46.809610   21825 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0228 16:14:46.847574   21825 docker.go:671] Got preloaded images: -- stdout --
nginx:latest
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0228 16:14:46.847611   21825 cache_images.go:84] Images are preloaded, skipping loading
I0228 16:14:46.847760   21825 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0228 16:14:47.300175   21825 cni.go:84] Creating CNI manager for ""
I0228 16:14:47.300190   21825 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0228 16:14:47.301899   21825 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0228 16:14:47.302019   21825 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0228 16:14:47.302188   21825 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0228 16:14:47.302248   21825 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0228 16:14:47.302322   21825 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0228 16:14:47.337151   21825 binaries.go:44] Found k8s binaries, skipping transfer
I0228 16:14:47.337239   21825 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0228 16:14:47.358842   21825 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0228 16:14:47.407843   21825 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0228 16:14:47.460456   21825 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I0228 16:14:47.516462   21825 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0228 16:14:47.533502   21825 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0228 16:14:47.572644   21825 certs.go:56] Setting up /home/mitrandir/.minikube/profiles/minikube for IP: 192.168.49.2
I0228 16:14:47.572681   21825 certs.go:190] acquiring lock for shared ca certs: {Name:mk325601f432fd1df1014795cb09cd50a5e77878 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0228 16:14:47.574480   21825 certs.go:199] skipping minikubeCA CA generation: /home/mitrandir/.minikube/ca.key
I0228 16:14:47.575656   21825 certs.go:199] skipping proxyClientCA CA generation: /home/mitrandir/.minikube/proxy-client-ca.key
I0228 16:14:47.576657   21825 certs.go:315] skipping minikube-user signed cert generation: /home/mitrandir/.minikube/profiles/minikube/client.key
I0228 16:14:47.577564   21825 certs.go:315] skipping minikube signed cert generation: /home/mitrandir/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0228 16:14:47.581333   21825 certs.go:315] skipping aggregator signed cert generation: /home/mitrandir/.minikube/profiles/minikube/proxy-client.key
I0228 16:14:47.581567   21825 certs.go:437] found cert: /home/mitrandir/.minikube/certs/home/mitrandir/.minikube/certs/ca-key.pem (1675 bytes)
I0228 16:14:47.581597   21825 certs.go:437] found cert: /home/mitrandir/.minikube/certs/home/mitrandir/.minikube/certs/ca.pem (1086 bytes)
I0228 16:14:47.581621   21825 certs.go:437] found cert: /home/mitrandir/.minikube/certs/home/mitrandir/.minikube/certs/cert.pem (1127 bytes)
I0228 16:14:47.581644   21825 certs.go:437] found cert: /home/mitrandir/.minikube/certs/home/mitrandir/.minikube/certs/key.pem (1675 bytes)
I0228 16:14:47.582486   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0228 16:14:47.655964   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0228 16:14:47.733834   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0228 16:14:47.817789   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0228 16:14:47.889931   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0228 16:14:47.968197   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0228 16:14:48.023748   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0228 16:14:48.083706   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0228 16:14:48.156246   21825 ssh_runner.go:362] scp /home/mitrandir/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0228 16:14:48.246163   21825 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0228 16:14:48.296382   21825 ssh_runner.go:195] Run: openssl version
I0228 16:14:48.322277   21825 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0228 16:14:48.353692   21825 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0228 16:14:48.365447   21825 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Feb 24 04:35 /usr/share/ca-certificates/minikubeCA.pem
I0228 16:14:48.365524   21825 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0228 16:14:48.382008   21825 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0228 16:14:48.406924   21825 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0228 16:14:48.419858   21825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0228 16:14:48.435114   21825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0228 16:14:48.453553   21825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0228 16:14:48.472061   21825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0228 16:14:48.486464   21825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0228 16:14:48.501491   21825 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0228 16:14:48.520162   21825 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/mitrandir:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0228 16:14:48.520500   21825 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0228 16:14:48.562810   21825 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0228 16:14:48.585623   21825 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I0228 16:14:48.585633   21825 kubeadm.go:636] restartCluster start
I0228 16:14:48.585689   21825 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0228 16:14:48.606884   21825 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0228 16:14:48.609056   21825 kubeconfig.go:92] found "minikube" server: "https://192.168.49.2:8443"
I0228 16:14:48.636739   21825 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0228 16:14:48.664489   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:48.664609   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:48.702982   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:48.702997   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:48.703078   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:48.737392   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:49.238102   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:49.238229   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:49.267259   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:49.738644   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:49.738726   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:49.764378   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:50.237829   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:50.238086   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:50.267197   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:50.738628   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:50.738763   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:50.766349   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:51.237861   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:51.237942   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:51.263711   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:51.738990   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:51.739251   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:51.775224   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:52.237706   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:52.237832   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:52.262507   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:52.737953   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:52.738028   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:52.756608   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:53.237507   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:53.237593   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:53.257568   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:53.737730   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:53.737811   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:53.757446   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:54.238415   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:54.238478   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:54.258725   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:54.737523   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:54.737626   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:54.758895   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:55.237545   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:55.237727   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:55.268202   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:55.738409   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:55.738499   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:55.756299   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:56.237509   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:56.237657   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:56.264950   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:56.738683   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:56.738753   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:56.758423   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:57.238556   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:57.238619   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:57.262106   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:57.738927   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:57.738999   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:57.762851   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:58.238603   21825 api_server.go:166] Checking apiserver status ...
I0228 16:14:58.238887   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0228 16:14:58.263642   21825 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0228 16:14:58.665580   21825 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I0228 16:14:58.665630   21825 kubeadm.go:1128] stopping kube-system containers ...
I0228 16:14:58.665771   21825 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0228 16:14:58.710566   21825 docker.go:469] Stopping containers: [80ff6913a604 33eeb9a305f3 df02b3be5633 cbc69a3fd748 027967a5ab05 7b814b24e164 095a53262434 67d912d7c9e3 b2dd5e584e27 27c15ff222c5 2617c05c93dc 0546ebf3ebea ebd442551a9b a5495d1b1676 f625a3e931f9 7b34dbc8cc9a 32187c2e0e00 7161525783e6 bd595f183fec 86a65fd8267e a026adf3a31f 5d02151e6057 6d3be11efc7f a2554dda4842 fe49795381ea c27a5000d61a 7c323c5799fc]
I0228 16:14:58.710645   21825 ssh_runner.go:195] Run: docker stop 80ff6913a604 33eeb9a305f3 df02b3be5633 cbc69a3fd748 027967a5ab05 7b814b24e164 095a53262434 67d912d7c9e3 b2dd5e584e27 27c15ff222c5 2617c05c93dc 0546ebf3ebea ebd442551a9b a5495d1b1676 f625a3e931f9 7b34dbc8cc9a 32187c2e0e00 7161525783e6 bd595f183fec 86a65fd8267e a026adf3a31f 5d02151e6057 6d3be11efc7f a2554dda4842 fe49795381ea c27a5000d61a 7c323c5799fc
I0228 16:14:58.755813   21825 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0228 16:14:58.786733   21825 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0228 16:14:58.811241   21825 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Feb 24 04:35 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Feb 28 07:24 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Feb 24 04:36 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Feb 28 07:24 /etc/kubernetes/scheduler.conf

I0228 16:14:58.811435   21825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0228 16:14:58.843527   21825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0228 16:14:58.863262   21825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0228 16:14:58.884866   21825 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I0228 16:14:58.884980   21825 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0228 16:14:58.903696   21825 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0228 16:14:58.923235   21825 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I0228 16:14:58.923319   21825 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0228 16:14:58.945318   21825 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0228 16:14:58.964406   21825 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0228 16:14:58.964418   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0228 16:14:59.365342   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0228 16:15:00.088590   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0228 16:15:00.415393   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0228 16:15:00.537878   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0228 16:15:00.638380   21825 api_server.go:52] waiting for apiserver process to appear ...
I0228 16:15:00.638458   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:00.665579   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:01.197570   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:01.697807   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:02.198335   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:02.697360   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:03.198072   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:03.697202   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:04.197238   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:04.305539   21825 api_server.go:72] duration metric: took 3.667160127s to wait for apiserver process to appear ...
I0228 16:15:04.305598   21825 api_server.go:88] waiting for apiserver healthz status ...
I0228 16:15:04.305618   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:04.306301   21825 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0228 16:15:04.306388   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:04.307093   21825 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0228 16:15:04.807867   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:04.808420   21825 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0228 16:15:05.308876   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:05.311805   21825 api_server.go:269] stopped: https://192.168.49.2:8443/healthz: Get "https://192.168.49.2:8443/healthz": dial tcp 192.168.49.2:8443: connect: connection refused
I0228 16:15:05.809888   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:08.485414   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0228 16:15:08.485433   21825 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0228 16:15:08.485444   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:08.528437   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0228 16:15:08.528454   21825 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0228 16:15:08.807432   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:08.812735   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0228 16:15:08.812759   21825 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0228 16:15:09.307915   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:09.314478   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0228 16:15:09.314499   21825 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0228 16:15:09.807496   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:09.823521   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W0228 16:15:09.823543   21825 api_server.go:103] status: https://192.168.49.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I0228 16:15:10.307633   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:10.312858   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0228 16:15:10.324433   21825 api_server.go:141] control plane version: v1.28.3
I0228 16:15:10.324456   21825 api_server.go:131] duration metric: took 6.018853407s to wait for apiserver health ...
I0228 16:15:10.324465   21825 cni.go:84] Creating CNI manager for ""
I0228 16:15:10.324482   21825 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0228 16:15:10.325976   21825 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I0228 16:15:10.327968   21825 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0228 16:15:10.380891   21825 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I0228 16:15:10.454637   21825 system_pods.go:43] waiting for kube-system pods to appear ...
I0228 16:15:10.474583   21825 system_pods.go:59] 7 kube-system pods found
I0228 16:15:10.474606   21825 system_pods.go:61] "coredns-5dd5756b68-666gf" [b0fd2987-7cd9-4f5d-8037-ea025aedeaa3] Running
I0228 16:15:10.474610   21825 system_pods.go:61] "etcd-minikube" [6901cc61-78a0-4b93-b124-d6c645d2e657] Running
I0228 16:15:10.474614   21825 system_pods.go:61] "kube-apiserver-minikube" [6d192c3e-fc65-4c28-b0f0-cc1fcc5516bf] Running
I0228 16:15:10.474619   21825 system_pods.go:61] "kube-controller-manager-minikube" [842ea9c9-50d3-43d1-b7e6-ccb8c76c83fc] Running
I0228 16:15:10.474624   21825 system_pods.go:61] "kube-proxy-jft4f" [15491826-21be-407b-9faf-bb7894864eb5] Running
I0228 16:15:10.474627   21825 system_pods.go:61] "kube-scheduler-minikube" [784f6a72-4c12-4c0c-8ec3-16e8a1e2e33b] Running
I0228 16:15:10.474631   21825 system_pods.go:61] "storage-provisioner" [6036baa5-36ff-42eb-bb86-c668c2296757] Running
I0228 16:15:10.474638   21825 system_pods.go:74] duration metric: took 19.98939ms to wait for pod list to return data ...
I0228 16:15:10.474647   21825 node_conditions.go:102] verifying NodePressure condition ...
I0228 16:15:10.484254   21825 node_conditions.go:122] node storage ephemeral capacity is 102106072Ki
I0228 16:15:10.484274   21825 node_conditions.go:123] node cpu capacity is 4
I0228 16:15:10.484293   21825 node_conditions.go:105] duration metric: took 9.642034ms to run NodePressure ...
I0228 16:15:10.484310   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0228 16:15:10.977522   21825 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0228 16:15:10.998890   21825 ops.go:34] apiserver oom_adj: -16
I0228 16:15:10.999120   21825 kubeadm.go:640] restartCluster took 22.413474259s
I0228 16:15:10.999130   21825 kubeadm.go:406] StartCluster complete in 22.478976693s
I0228 16:15:10.999152   21825 settings.go:142] acquiring lock: {Name:mkf1500aa7da94027b4c332395643c9231fa5981 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0228 16:15:11.002217   21825 settings.go:150] Updating kubeconfig:  /home/mitrandir/.kube/config
I0228 16:15:11.006368   21825 lock.go:35] WriteFile acquiring /home/mitrandir/.kube/config: {Name:mk27692eaa433dc56bbf3d8c4daeb385b39d1e8f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0228 16:15:11.008687   21825 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0228 16:15:11.009133   21825 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0228 16:15:11.009185   21825 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0228 16:15:11.009263   21825 addons.go:69] Setting dashboard=true in profile "minikube"
I0228 16:15:11.009272   21825 addons.go:231] Setting addon dashboard=true in "minikube"
W0228 16:15:11.009278   21825 addons.go:240] addon dashboard should already be in state true
I0228 16:15:11.009346   21825 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W0228 16:15:11.009351   21825 addons.go:240] addon storage-provisioner should already be in state true
I0228 16:15:11.009370   21825 host.go:66] Checking if "minikube" exists ...
I0228 16:15:11.009372   21825 host.go:66] Checking if "minikube" exists ...
I0228 16:15:11.010527   21825 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0228 16:15:11.010610   21825 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0228 16:15:11.010621   21825 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0228 16:15:11.011468   21825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0228 16:15:11.011993   21825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0228 16:15:11.014351   21825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0228 16:15:11.016380   21825 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0228 16:15:11.016405   21825 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0228 16:15:11.018734   21825 out.go:177] üîé  Verifying Kubernetes components...
I0228 16:15:11.020606   21825 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0228 16:15:11.099130   21825 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0228 16:15:11.102019   21825 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0228 16:15:11.102032   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0228 16:15:11.102198   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:15:11.128841   21825 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/dashboard:v2.7.0
I0228 16:15:11.131184   21825 out.go:177]     ‚ñ™ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0228 16:15:11.132501   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0228 16:15:11.132515   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0228 16:15:11.132714   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:15:11.170898   21825 addons.go:231] Setting addon default-storageclass=true in "minikube"
W0228 16:15:11.170986   21825 addons.go:240] addon default-storageclass should already be in state true
I0228 16:15:11.171011   21825 host.go:66] Checking if "minikube" exists ...
I0228 16:15:11.171410   21825 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0228 16:15:11.211313   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:15:11.219104   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:15:11.266094   21825 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0228 16:15:11.266105   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0228 16:15:11.266160   21825 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0228 16:15:11.463899   21825 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/mitrandir/.minikube/machines/minikube/id_rsa Username:docker}
I0228 16:15:11.552684   21825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0228 16:15:11.789790   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0228 16:15:11.789809   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0228 16:15:12.064879   21825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0228 16:15:12.106703   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0228 16:15:12.106764   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0228 16:15:12.423283   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0228 16:15:12.423298   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0228 16:15:12.647378   21825 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.638661454s)
I0228 16:15:12.647439   21825 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0228 16:15:12.647478   21825 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.626859857s)
I0228 16:15:12.647507   21825 api_server.go:52] waiting for apiserver process to appear ...
I0228 16:15:12.647758   21825 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0228 16:15:12.799258   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0228 16:15:12.799270   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0228 16:15:12.998284   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I0228 16:15:12.998302   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0228 16:15:13.110112   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0228 16:15:13.110130   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0228 16:15:13.396720   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0228 16:15:13.396743   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0228 16:15:13.571124   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0228 16:15:13.571139   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0228 16:15:13.701055   21825 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0228 16:15:13.701069   21825 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0228 16:15:13.811828   21825 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0228 16:15:15.744272   21825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.679368682s)
I0228 16:15:15.744408   21825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (4.191707012s)
I0228 16:15:15.744474   21825 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.096705631s)
I0228 16:15:15.744486   21825 api_server.go:72] duration metric: took 4.728012902s to wait for apiserver process to appear ...
I0228 16:15:15.744553   21825 api_server.go:88] waiting for apiserver healthz status ...
I0228 16:15:15.744567   21825 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0228 16:15:15.753675   21825 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I0228 16:15:15.761233   21825 api_server.go:141] control plane version: v1.28.3
I0228 16:15:15.761253   21825 api_server.go:131] duration metric: took 16.694603ms to wait for apiserver health ...
I0228 16:15:15.761259   21825 system_pods.go:43] waiting for kube-system pods to appear ...
I0228 16:15:15.798996   21825 system_pods.go:59] 7 kube-system pods found
I0228 16:15:15.799019   21825 system_pods.go:61] "coredns-5dd5756b68-666gf" [b0fd2987-7cd9-4f5d-8037-ea025aedeaa3] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0228 16:15:15.799024   21825 system_pods.go:61] "etcd-minikube" [6901cc61-78a0-4b93-b124-d6c645d2e657] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0228 16:15:15.799028   21825 system_pods.go:61] "kube-apiserver-minikube" [6d192c3e-fc65-4c28-b0f0-cc1fcc5516bf] Running
I0228 16:15:15.799031   21825 system_pods.go:61] "kube-controller-manager-minikube" [842ea9c9-50d3-43d1-b7e6-ccb8c76c83fc] Running
I0228 16:15:15.799036   21825 system_pods.go:61] "kube-proxy-jft4f" [15491826-21be-407b-9faf-bb7894864eb5] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0228 16:15:15.799040   21825 system_pods.go:61] "kube-scheduler-minikube" [784f6a72-4c12-4c0c-8ec3-16e8a1e2e33b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0228 16:15:15.799043   21825 system_pods.go:61] "storage-provisioner" [6036baa5-36ff-42eb-bb86-c668c2296757] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0228 16:15:15.799049   21825 system_pods.go:74] duration metric: took 37.786376ms to wait for pod list to return data ...
I0228 16:15:15.799057   21825 kubeadm.go:581] duration metric: took 4.782584641s to wait for : map[apiserver:true system_pods:true] ...
I0228 16:15:15.799072   21825 node_conditions.go:102] verifying NodePressure condition ...
I0228 16:15:15.813202   21825 node_conditions.go:122] node storage ephemeral capacity is 102106072Ki
I0228 16:15:15.813219   21825 node_conditions.go:123] node cpu capacity is 4
I0228 16:15:15.813228   21825 node_conditions.go:105] duration metric: took 14.153202ms to run NodePressure ...
I0228 16:15:15.813293   21825 start.go:228] waiting for startup goroutines ...
I0228 16:15:16.788916   21825 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.977057706s)
I0228 16:15:16.790923   21825 out.go:177] üí°  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I0228 16:15:16.793653   21825 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0228 16:15:16.795546   21825 addons.go:502] enable addons completed in 5.786410141s: enabled=[storage-provisioner default-storageclass dashboard]
I0228 16:15:16.795585   21825 start.go:233] waiting for cluster config update ...
I0228 16:15:16.795596   21825 start.go:242] writing updated cluster config ...
I0228 16:15:16.796415   21825 ssh_runner.go:195] Run: rm -f paused
I0228 16:15:17.663565   21825 start.go:600] kubectl: 1.29.2, cluster: 1.28.3 (minor skew: 1)
I0228 16:15:17.665191   21825 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Feb 28 22:44:08 minikube dockerd[795]: time="2024-02-28T22:44:08.016322634Z" level=info msg="ignoring event" container=f31ce8879c8b0d2d1232e750a559d4793367ad43382d1ad5294a173f66ee92a9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 22:44:08 minikube dockerd[795]: time="2024-02-28T22:44:08.241416350Z" level=info msg="ignoring event" container=3b705ed4738f5daf7cd25b87fed5bb4b16d17a880bf6b277ea830bc01f92d241 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 22:44:08 minikube cri-dockerd[1012]: time="2024-02-28T22:44:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/69d436fbf4e652c88289e42d1da7a1fbbff8d876fca86b80fe49464389738a8a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 22:44:08 minikube cri-dockerd[1012]: time="2024-02-28T22:44:08Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 22:44:09 minikube dockerd[795]: time="2024-02-28T22:44:09.053060449Z" level=info msg="ignoring event" container=794b2aea64426bab3fac669b9ea521d8ed4fdca1616460a60913e87b342db755 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 22:44:09 minikube dockerd[795]: time="2024-02-28T22:44:09.219133475Z" level=info msg="ignoring event" container=73b956000ea7a9fda968182bd12208cbb0c6959ab570689bd456ad99c66ff575 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 22:44:10 minikube cri-dockerd[1012]: time="2024-02-28T22:44:10Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 22:44:11 minikube dockerd[795]: time="2024-02-28T22:44:11.187079027Z" level=info msg="ignoring event" container=ad622bdc0732e71a1e46b9ce60f55cb03e331def212553fe5aa4b7c6548002e7 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 22:44:11 minikube dockerd[795]: time="2024-02-28T22:44:11.427559244Z" level=info msg="ignoring event" container=2ef36b4fcaff4f2db37a46c47779fc90969e46cbc4e78169cb6a37ab488ed12a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 22:44:12 minikube cri-dockerd[1012]: time="2024-02-28T22:44:12Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 23:06:02 minikube dockerd[795]: time="2024-02-28T23:06:02.767493167Z" level=info msg="ignoring event" container=a0b146485ca15a67a1a73c21e3caa48b11563a79333adb7b972a7af10de154c5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:03 minikube dockerd[795]: time="2024-02-28T23:06:03.050323531Z" level=info msg="ignoring event" container=0abd6751b45dacd69d635d8f284680ed6b3ee3085093c0d23d1b81c77dbfd913 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:03 minikube cri-dockerd[1012]: time="2024-02-28T23:06:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4e973455b1311bc66e4eb0af068e48dde682b55b46b8cf77dda75d1a20c723b8/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:06:03 minikube cri-dockerd[1012]: time="2024-02-28T23:06:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0ab8df4ffaf6a38bde41cb2b210210c91bf323a52a7fe29690f1de25750037da/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:06:03 minikube cri-dockerd[1012]: time="2024-02-28T23:06:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c5dd30c8380a590146eff8f14cff77d8b22fe2a3a1dba784aa2fbc7ca3ef72ca/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:06:15 minikube cri-dockerd[1012]: time="2024-02-28T23:06:15Z" level=info msg="Pulling image nginx:1.18: f7ec5a41d630: Extracting [================================================>  ]  26.25MB/27.14MB"
Feb 28 23:06:17 minikube cri-dockerd[1012]: time="2024-02-28T23:06:17Z" level=info msg="Stop pulling image nginx:1.18: Status: Downloaded newer image for nginx:1.18"
Feb 28 23:06:19 minikube dockerd[795]: time="2024-02-28T23:06:19.262470341Z" level=info msg="ignoring event" container=3a02dbccad4598b42bcb63ce019709cde3eded39b4ae6a7facf87741c8ed22d2 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:19 minikube dockerd[795]: time="2024-02-28T23:06:19.488065452Z" level=info msg="ignoring event" container=889378e70d8a9f136c7df938b0f04b28c981404589b5cfadb5737d524da09dd6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:19 minikube cri-dockerd[1012]: time="2024-02-28T23:06:19Z" level=info msg="Stop pulling image nginx:1.18: Status: Image is up to date for nginx:1.18"
Feb 28 23:06:19 minikube cri-dockerd[1012]: time="2024-02-28T23:06:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db0ca8b28d68c061d6a2dedf81e4158d74aa8f8ca9c26c2078153cb9be28d6d0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:06:20 minikube dockerd[795]: time="2024-02-28T23:06:20.305502838Z" level=info msg="ignoring event" container=a4ff4db1b019347de088618ab5c7fb07ce25672ad04bdba1f880af110e737200 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:20 minikube dockerd[795]: time="2024-02-28T23:06:20.534487782Z" level=info msg="ignoring event" container=69d436fbf4e652c88289e42d1da7a1fbbff8d876fca86b80fe49464389738a8a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:20 minikube cri-dockerd[1012]: time="2024-02-28T23:06:20Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d9d973d5c4ebdd62778ce8e6ddb8b510620ad9f3f75d7d4593c401182d697a54/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:06:21 minikube cri-dockerd[1012]: time="2024-02-28T23:06:21Z" level=info msg="Stop pulling image nginx:1.18: Status: Image is up to date for nginx:1.18"
Feb 28 23:06:22 minikube dockerd[795]: time="2024-02-28T23:06:22.401909790Z" level=info msg="ignoring event" container=d81f9c483ac655ce225acd9d2f73196b543a07b805c8bb20577e316de0b421c4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:22 minikube dockerd[795]: time="2024-02-28T23:06:22.636790688Z" level=info msg="ignoring event" container=bbf9cbdd95d0b25f59ba2ec3f61de313bafd9a17162d6c2fc31c766e8e35a015 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:22 minikube cri-dockerd[1012]: time="2024-02-28T23:06:22Z" level=info msg="Stop pulling image nginx:1.18: Status: Image is up to date for nginx:1.18"
Feb 28 23:06:22 minikube cri-dockerd[1012]: time="2024-02-28T23:06:22Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a955ef2dbeac09856c3fbd32adca99a81d2bf024b903999c863b25ed8f96df55/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:06:23 minikube dockerd[795]: time="2024-02-28T23:06:23.453297147Z" level=info msg="ignoring event" container=934b8d106feb31ff031fa0e53a87150b96ca080544595127a87638bc74cd0568 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:23 minikube dockerd[795]: time="2024-02-28T23:06:23.663471593Z" level=info msg="ignoring event" container=dd7f29c50cae10cb211ce91771dc1fc1e5027e14b236c55ed55a41fa6d6088e0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:24 minikube cri-dockerd[1012]: time="2024-02-28T23:06:24Z" level=info msg="Stop pulling image nginx:1.18: Status: Image is up to date for nginx:1.18"
Feb 28 23:06:25 minikube dockerd[795]: time="2024-02-28T23:06:25.519673328Z" level=info msg="ignoring event" container=b2a854fb338802df5dbb4adc01a63a40eff96c0fdfdaee80953aca618aa001b9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:25 minikube dockerd[795]: time="2024-02-28T23:06:25.768578143Z" level=info msg="ignoring event" container=b54ffad78f465ba9f81206e8c14d79509d7b4bc956bf71f82f86a785914bb142 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:06:26 minikube cri-dockerd[1012]: time="2024-02-28T23:06:26Z" level=info msg="Stop pulling image nginx:1.18: Status: Image is up to date for nginx:1.18"
Feb 28 23:11:35 minikube dockerd[795]: time="2024-02-28T23:11:35.510045620Z" level=info msg="ignoring event" container=80fc415c8f54f4f01fab9e7f9ee335e0d61f821332ef3561cbe7eb90b95f1c09 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:35 minikube cri-dockerd[1012]: time="2024-02-28T23:11:35Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"app-deploy-7f6445c8ff-hdlbn_default\": unexpected command output nsenter: cannot open /proc/77310/ns/net: No such file or directory\n with error: exit status 1"
Feb 28 23:11:35 minikube dockerd[795]: time="2024-02-28T23:11:35.863825319Z" level=info msg="ignoring event" container=a955ef2dbeac09856c3fbd32adca99a81d2bf024b903999c863b25ed8f96df55 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:36 minikube cri-dockerd[1012]: time="2024-02-28T23:11:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4f97cb0ab1e5340000fe09be3847f712fb3fd1e2a820c10c8adc1511a0afcb9b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:11:36 minikube cri-dockerd[1012]: time="2024-02-28T23:11:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cb853218be35447e1a273681bffb1112fbbd296bf40988c14adb4fc983f1e54e/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:11:36 minikube cri-dockerd[1012]: time="2024-02-28T23:11:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d79a5b7359c6e41ce2e280d8a2ac73f8dead82f80db62aec02621471bbd8e714/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:11:38 minikube cri-dockerd[1012]: time="2024-02-28T23:11:38Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 23:11:39 minikube dockerd[795]: time="2024-02-28T23:11:39.313480879Z" level=info msg="ignoring event" container=3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:39 minikube dockerd[795]: time="2024-02-28T23:11:39.490332420Z" level=info msg="ignoring event" container=db0ca8b28d68c061d6a2dedf81e4158d74aa8f8ca9c26c2078153cb9be28d6d0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:39 minikube cri-dockerd[1012]: time="2024-02-28T23:11:39Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 23:11:39 minikube cri-dockerd[1012]: time="2024-02-28T23:11:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3450ce5166c740f59a86a4a81618f2c8d3fe3052c958ea58b9af8d46026b1420/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:11:40 minikube dockerd[795]: time="2024-02-28T23:11:40.372706278Z" level=info msg="ignoring event" container=00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:40 minikube dockerd[795]: time="2024-02-28T23:11:40.568675925Z" level=info msg="ignoring event" container=c5dd30c8380a590146eff8f14cff77d8b22fe2a3a1dba784aa2fbc7ca3ef72ca module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:40 minikube cri-dockerd[1012]: time="2024-02-28T23:11:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e88ec8504c8dc473b0f68eb02122eadf89c65df42157bc1bd6dc7ac2653ec94c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:11:41 minikube cri-dockerd[1012]: time="2024-02-28T23:11:41Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 23:11:42 minikube dockerd[795]: time="2024-02-28T23:11:42.458280526Z" level=info msg="ignoring event" container=e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:42 minikube dockerd[795]: time="2024-02-28T23:11:42.666798598Z" level=info msg="ignoring event" container=0ab8df4ffaf6a38bde41cb2b210210c91bf323a52a7fe29690f1de25750037da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:42 minikube cri-dockerd[1012]: time="2024-02-28T23:11:42Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 23:11:43 minikube cri-dockerd[1012]: time="2024-02-28T23:11:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/118135b347d0bc217b203f8dd562f2aad6923ca1ab9061085a13e97eb07499d2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local localdomain options ndots:5]"
Feb 28 23:11:43 minikube dockerd[795]: time="2024-02-28T23:11:43.565345193Z" level=info msg="ignoring event" container=675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:43 minikube dockerd[795]: time="2024-02-28T23:11:43.749096685Z" level=info msg="ignoring event" container=d9d973d5c4ebdd62778ce8e6ddb8b510620ad9f3f75d7d4593c401182d697a54 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:44 minikube cri-dockerd[1012]: time="2024-02-28T23:11:44Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"
Feb 28 23:11:45 minikube dockerd[795]: time="2024-02-28T23:11:45.660401117Z" level=info msg="ignoring event" container=f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:45 minikube dockerd[795]: time="2024-02-28T23:11:45.889460976Z" level=info msg="ignoring event" container=4e973455b1311bc66e4eb0af068e48dde682b55b46b8cf77dda75d1a20c723b8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 28 23:11:45 minikube cri-dockerd[1012]: time="2024-02-28T23:11:45Z" level=info msg="Stop pulling image nginx:1.24: Status: Image is up to date for nginx:1.24"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
74fae85732664       nginx@sha256:ee187e563496b690edaab157f89db924cd35fab42631309f4d62957baecf7d6c   14 minutes ago      Running             nginx-container             0                   118135b347d0b       app-deploy-7f7d8f446-gjp7j
f29cdb9b24d79       nginx@sha256:ee187e563496b690edaab157f89db924cd35fab42631309f4d62957baecf7d6c   14 minutes ago      Running             nginx-container             0                   e88ec8504c8dc       app-deploy-7f7d8f446-z928z
4dd799a697130       nginx@sha256:ee187e563496b690edaab157f89db924cd35fab42631309f4d62957baecf7d6c   14 minutes ago      Running             nginx-container             0                   3450ce5166c74       app-deploy-7f7d8f446-jhtms
acc1b1c2dbaa2       nginx@sha256:ee187e563496b690edaab157f89db924cd35fab42631309f4d62957baecf7d6c   14 minutes ago      Running             nginx-container             0                   d79a5b7359c6e       app-deploy-7f7d8f446-k4c7r
43d496a763373       nginx@sha256:ee187e563496b690edaab157f89db924cd35fab42631309f4d62957baecf7d6c   14 minutes ago      Running             nginx-container             0                   cb853218be354       app-deploy-7f7d8f446-4pkj2
05cf14c453162       nginx@sha256:ee187e563496b690edaab157f89db924cd35fab42631309f4d62957baecf7d6c   14 minutes ago      Running             nginx-container             0                   4f97cb0ab1e53       app-deploy-7f7d8f446-bd9f4
65a85133b9bd3       6e38f40d628db                                                                   About an hour ago   Running             storage-provisioner         16                  e03d18d89fa33       storage-provisioner
bc238ac3617bf       nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107   About an hour ago   Running             nginx-container             5                   016dd924812be       pod-nginx
a84787069502c       115053965e86b                                                                   About an hour ago   Running             dashboard-metrics-scraper   7                   eabbbf7241585       dashboard-metrics-scraper-7fd5cb4ddc-tvcrm
12e0f6179dc76       ead0a4a53df89                                                                   About an hour ago   Running             coredns                     7                   0bac50f77d41c       coredns-5dd5756b68-666gf
5f12cdc9f3410       07655ddf2eebe                                                                   About an hour ago   Running             kubernetes-dashboard        12                  96ad1f14d4e5b       kubernetes-dashboard-8694d4445c-vqv8w
0f649d71d2ecb       bfc896cf80fba                                                                   About an hour ago   Running             kube-proxy                  7                   429021656e9e1       kube-proxy-jft4f
0bb15cd0daf49       6e38f40d628db                                                                   About an hour ago   Exited              storage-provisioner         15                  e03d18d89fa33       storage-provisioner
b287a5ee23e56       6d1b4fd1b182d                                                                   About an hour ago   Running             kube-scheduler              7                   81e58c4fc5987       kube-scheduler-minikube
c3c82d28f09ef       73deb9a3f7025                                                                   About an hour ago   Running             etcd                        7                   c3a1b12117051       etcd-minikube
f1782f317d4d9       10baa1ca17068                                                                   About an hour ago   Running             kube-controller-manager     7                   8075f9d2a11ea       kube-controller-manager-minikube
96e3bcc8c0ec7       5374347291230                                                                   About an hour ago   Running             kube-apiserver              7                   11d24f1ddd666       kube-apiserver-minikube
1bc7208b07312       07655ddf2eebe                                                                   16 hours ago        Exited              kubernetes-dashboard        11                  9af0d9985cb8d       kubernetes-dashboard-8694d4445c-vqv8w
557941318e0d1       nginx@sha256:c26ae7472d624ba1fafd296e73cecc4f93f853088e6a9c13c0d52f6ca5865107   16 hours ago        Exited              nginx-container             4                   d58f547584018       pod-nginx
cb241648b0194       115053965e86b                                                                   16 hours ago        Exited              dashboard-metrics-scraper   6                   3638af7fb659e       dashboard-metrics-scraper-7fd5cb4ddc-tvcrm
33eeb9a305f39       ead0a4a53df89                                                                   16 hours ago        Exited              coredns                     6                   027967a5ab05a       coredns-5dd5756b68-666gf
df02b3be56339       bfc896cf80fba                                                                   16 hours ago        Exited              kube-proxy                  6                   7b814b24e1644       kube-proxy-jft4f
67d912d7c9e31       6d1b4fd1b182d                                                                   16 hours ago        Exited              kube-scheduler              6                   a5495d1b16760       kube-scheduler-minikube
b2dd5e584e273       5374347291230                                                                   16 hours ago        Exited              kube-apiserver              6                   ebd442551a9ba       kube-apiserver-minikube
27c15ff222c56       73deb9a3f7025                                                                   16 hours ago        Exited              etcd                        6                   0546ebf3ebead       etcd-minikube
2617c05c93dc7       10baa1ca17068                                                                   16 hours ago        Exited              kube-controller-manager     6                   f625a3e931f99       kube-controller-manager-minikube

* 
* ==> coredns [12e0f6179dc7] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:52685 - 21618 "HINFO IN 8130489470455207783.7595479645438060238. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.175854072s

* 
* ==> coredns [33eeb9a305f3] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:44757 - 46617 "HINFO IN 8926217627815100477.1849877703986964169. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.082627153s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_02_23T22_36_04_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 24 Feb 2024 04:35:59 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 28 Feb 2024 23:25:54 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 28 Feb 2024 23:21:52 +0000   Sat, 24 Feb 2024 04:35:55 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 28 Feb 2024 23:21:52 +0000   Sat, 24 Feb 2024 04:35:55 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 28 Feb 2024 23:21:52 +0000   Sat, 24 Feb 2024 04:35:55 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 28 Feb 2024 23:21:52 +0000   Sat, 24 Feb 2024 04:36:15 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  102106072Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8086168Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  102106072Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8086168Ki
  pods:               110
System Info:
  Machine ID:                 3607da49b2d247f2a0db9d82c580c71f
  System UUID:                3847c167-6b74-48b5-8b67-b731873c2faf
  Boot ID:                    704b5edf-89f5-49f0-b89b-a5ff246c0293
  Kernel Version:             6.5.0-21-generic
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (16 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     app-deploy-7f7d8f446-4pkj2                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  default                     app-deploy-7f7d8f446-bd9f4                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  default                     app-deploy-7f7d8f446-gjp7j                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  default                     app-deploy-7f7d8f446-jhtms                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  default                     app-deploy-7f7d8f446-k4c7r                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  default                     app-deploy-7f7d8f446-z928z                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         14m
  default                     pod-nginx                                     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d16h
  kube-system                 coredns-5dd5756b68-666gf                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     4d18h
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         4d18h
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kube-system                 kube-proxy-jft4f                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-tvcrm    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-vqv8w         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Feb28 21:31] [Firmware Bug]: TSC doesn't count with P0 frequency!
[  +0.000000] unchecked MSR access error: RDMSR from 0x852 at rIP: 0xffffffff960bb377 (native_read_msr+0x7/0x50)
[  +0.000008] Call Trace:
[  +0.000001]  <TASK>
[  +0.000003]  ? show_stack_regs+0x23/0x40
[  +0.000004]  ? ex_handler_msr+0x164/0x190
[  +0.000002]  ? fixup_exception+0x86/0x370
[  +0.000001]  ? gp_try_fixup_and_notify+0x23/0xe0
[  +0.000003]  ? exc_general_protection+0x146/0x460
[  +0.000005]  ? asm_exc_general_protection+0x27/0x30
[  +0.000003]  ? __pfx_native_apic_msr_read+0x10/0x10
[  +0.000003]  ? native_read_msr+0x7/0x50
[  +0.000001]  native_apic_msr_read+0x1b/0x60
[  +0.000002]  setup_APIC_eilvt+0x51/0x220
[  +0.000003]  mce_amd_feature_init+0x2e6/0x390
[  +0.000003]  __mcheck_cpu_init_vendor+0x76/0x100
[  +0.000003]  mcheck_cpu_init+0x12f/0x2c0
[  +0.000001]  identify_cpu+0x380/0x630
[  +0.000003]  identify_boot_cpu+0x10/0xc0
[  +0.000004]  arch_cpu_finalize_init+0x9/0x70
[  +0.000002]  start_kernel+0x2c8/0x440
[  +0.000002]  x86_64_start_reservations+0x18/0x30
[  +0.000003]  x86_64_start_kernel+0xa4/0xe0
[  +0.000002]  secondary_startup_64_no_verify+0x17e/0x18b
[  +0.000003]  </TASK>
[  +0.000002] [Firmware Bug]: cpu 0, try to use APIC520 (LVT offset 2) for vector 0xf4, but the register is already in use for vector 0x0 on this cpu
[  +0.002941] Speculative Return Stack Overflow: IBPB-extending microcode not applied!
[  +0.000000] Speculative Return Stack Overflow: WARNING: See https://kernel.org/doc/html/latest/admin-guide/hw-vuln/srso.html for mitigation options.
[  -1.111371] [Firmware Bug]: cpu 1, try to use APIC520 (LVT offset 2) for vector 0xf4, but the register is already in use for vector 0x0 on this cpu
[  +0.000000] [Firmware Bug]: cpu 2, try to use APIC520 (LVT offset 2) for vector 0xf4, but the register is already in use for vector 0x0 on this cpu
[  +0.000000] [Firmware Bug]: cpu 3, try to use APIC520 (LVT offset 2) for vector 0xf4, but the register is already in use for vector 0x0 on this cpu
[  +3.485204] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.000309] platform eisa.0: EISA: Cannot allocate resource for mainboard
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 1
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 2
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 3
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 4
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 5
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 6
[  +0.000003] platform eisa.0: Cannot allocate resource for EISA slot 7
[  +0.000002] platform eisa.0: Cannot allocate resource for EISA slot 8
[  +0.000006] amd_pstate: the _CPC object is not present in SBIOS or ACPI disabled
[  +2.450984] piix4_smbus 0000:00:07.3: SMBus Host Controller not enabled!
[  +0.610673] sd 32:0:0:0: [sda] Assuming drive cache: write through
[  +0.967175] systemd[1]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.054829] block sda: the capability attribute has been deprecated.
[  +3.484234] Bluetooth: hci0: unexpected cc 0x0c12 length: 2 < 3
[  +0.000020] Bluetooth: hci0: Opcode 0x c12 failed: -38
[  +3.314834] workqueue: pcpu_balance_workfn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[  +3.445761] kauditd_printk_skb: 50 callbacks suppressed
[Feb28 22:33] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.
[Feb28 23:18] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND

* 
* ==> etcd [27c15ff222c5] <==
* {"level":"info","ts":"2024-02-28T07:24:30.786754Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-28T07:24:30.786795Z","caller":"embed/etcd.go:495","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-28T07:24:30.797785Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-02-28T07:24:30.807962Z","caller":"embed/etcd.go:309","msg":"starting an etcd server","etcd-version":"3.5.9","git-sha":"bdbbde998","go-version":"go1.19.9","go-os":"linux","go-arch":"amd64","max-cpu-set":4,"max-cpu-available":4,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-02-28T07:24:30.951096Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"140.126094ms"}
{"level":"info","ts":"2024-02-28T07:24:31.885751Z","caller":"etcdserver/server.go:509","msg":"recovered v2 store from snapshot","snapshot-index":30003,"snapshot-size":"8.9 kB"}
{"level":"info","ts":"2024-02-28T07:24:31.886173Z","caller":"etcdserver/server.go:522","msg":"recovered v3 backend from snapshot","backend-size-bytes":3227648,"backend-size":"3.2 MB","backend-size-in-use-bytes":1388544,"backend-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-02-28T07:24:32.381039Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":36662}
{"level":"info","ts":"2024-02-28T07:24:32.387363Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-02-28T07:24:32.388429Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 7"}
{"level":"info","ts":"2024-02-28T07:24:32.388493Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [aec36adc501070cc], term: 7, commit: 36662, applied: 30003, lastindex: 36662, lastterm: 7]"}
{"level":"info","ts":"2024-02-28T07:24:32.388581Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-28T07:24:32.389462Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","recovered-remote-peer-id":"aec36adc501070cc","recovered-remote-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-02-28T07:24:32.389499Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2024-02-28T07:24:32.391143Z","caller":"auth/store.go:1238","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-02-28T07:24:32.39223Z","caller":"mvcc/kvstore.go:323","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":29007}
{"level":"info","ts":"2024-02-28T07:24:32.424712Z","caller":"mvcc/kvstore.go:393","msg":"kvstore restored","current-rev":29415}
{"level":"info","ts":"2024-02-28T07:24:32.425971Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-02-28T07:24:32.427756Z","caller":"etcdserver/corrupt.go:95","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-02-28T07:24:32.428528Z","caller":"etcdserver/corrupt.go:165","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-28T07:24:32.428858Z","caller":"etcdserver/server.go:845","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.9","cluster-id":"fa54960ea34d58be","cluster-version":"3.5"}
{"level":"info","ts":"2024-02-28T07:24:32.42946Z","caller":"etcdserver/server.go:738","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-02-28T07:24:32.429627Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-28T07:24:32.435287Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-28T07:24:32.435301Z","caller":"fileutil/purge.go:44","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-02-28T07:24:32.43775Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-02-28T07:24:32.437999Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-28T07:24:32.438057Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-28T07:24:32.438136Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-28T07:24:32.454067Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-28T07:24:33.189912Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 7"}
{"level":"info","ts":"2024-02-28T07:24:33.190006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 7"}
{"level":"info","ts":"2024-02-28T07:24:33.190026Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 7"}
{"level":"info","ts":"2024-02-28T07:24:33.190037Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 8"}
{"level":"info","ts":"2024-02-28T07:24:33.190042Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-28T07:24:33.190049Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 8"}
{"level":"info","ts":"2024-02-28T07:24:33.190054Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-28T07:24:33.191042Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-28T07:24:33.191042Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-28T07:24:33.19112Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-28T07:24:33.191547Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-28T07:24:33.191665Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-28T07:24:33.192276Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-28T07:24:33.192897Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-28T07:34:33.221565Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29809}
{"level":"info","ts":"2024-02-28T07:34:33.236869Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":29809,"took":"15.059193ms","hash":2385106073}
{"level":"info","ts":"2024-02-28T07:34:33.236935Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2385106073,"revision":29809,"compact-revision":29007}
{"level":"info","ts":"2024-02-28T07:39:33.226854Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30086}
{"level":"info","ts":"2024-02-28T07:39:33.227567Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":30086,"took":"510.194¬µs","hash":3557558974}
{"level":"info","ts":"2024-02-28T07:39:33.227616Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3557558974,"revision":30086,"compact-revision":29809}
{"level":"info","ts":"2024-02-28T07:43:25.24887Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-02-28T07:43:25.249004Z","caller":"embed/etcd.go:376","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-02-28T07:43:25.249083Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-28T07:43:25.249163Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-28T07:43:25.482034Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-02-28T07:43:25.482081Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-02-28T07:43:25.482119Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-02-28T07:43:25.484999Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-28T07:43:25.485136Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-28T07:43:25.485142Z","caller":"embed/etcd.go:378","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}

* 
* ==> etcd [c3c82d28f09e] <==
* {"level":"info","ts":"2024-02-28T22:15:06.028497Z","caller":"embed/etcd.go:278","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-02-28T22:15:06.028642Z","caller":"embed/etcd.go:855","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-02-28T22:15:06.030052Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-28T22:15:06.030321Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-02-28T22:15:06.900925Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 8"}
{"level":"info","ts":"2024-02-28T22:15:06.901069Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 8"}
{"level":"info","ts":"2024-02-28T22:15:06.90114Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 8"}
{"level":"info","ts":"2024-02-28T22:15:06.90118Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 9"}
{"level":"info","ts":"2024-02-28T22:15:06.901189Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-02-28T22:15:06.901206Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 9"}
{"level":"info","ts":"2024-02-28T22:15:06.901213Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 9"}
{"level":"info","ts":"2024-02-28T22:15:06.903321Z","caller":"etcdserver/server.go:2062","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-02-28T22:15:06.903585Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-28T22:15:06.903553Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-02-28T22:15:06.904142Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-02-28T22:15:06.90426Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-02-28T22:15:06.904803Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-02-28T22:15:06.905501Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-02-28T22:25:06.93419Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31005}
{"level":"info","ts":"2024-02-28T22:25:06.950236Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31005,"took":"15.670489ms","hash":682787516}
{"level":"info","ts":"2024-02-28T22:25:06.950305Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":682787516,"revision":31005,"compact-revision":30086}
{"level":"info","ts":"2024-02-28T22:30:06.939772Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31325}
{"level":"info","ts":"2024-02-28T22:30:06.940921Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31325,"took":"666.397¬µs","hash":965998553}
{"level":"info","ts":"2024-02-28T22:30:06.941273Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":965998553,"revision":31325,"compact-revision":31005}
{"level":"info","ts":"2024-02-28T22:35:06.94702Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31564}
{"level":"info","ts":"2024-02-28T22:35:06.949114Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31564,"took":"805.267¬µs","hash":2592006162}
{"level":"info","ts":"2024-02-28T22:35:06.949299Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2592006162,"revision":31564,"compact-revision":31325}
{"level":"info","ts":"2024-02-28T22:40:06.953327Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31923}
{"level":"info","ts":"2024-02-28T22:40:06.954664Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":31923,"took":"850.141¬µs","hash":166164760}
{"level":"info","ts":"2024-02-28T22:40:06.95471Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":166164760,"revision":31923,"compact-revision":31564}
{"level":"info","ts":"2024-02-28T22:40:20.773729Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":40004,"local-member-snapshot-index":30003,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-02-28T22:40:20.77753Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":40004}
{"level":"info","ts":"2024-02-28T22:40:20.777753Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":35004}
{"level":"info","ts":"2024-02-28T22:45:06.958133Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32162}
{"level":"info","ts":"2024-02-28T22:45:06.959986Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32162,"took":"1.03651ms","hash":819093829}
{"level":"info","ts":"2024-02-28T22:45:06.960033Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":819093829,"revision":32162,"compact-revision":31923}
{"level":"info","ts":"2024-02-28T22:50:06.964445Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32581}
{"level":"info","ts":"2024-02-28T22:50:06.98044Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32581,"took":"15.228178ms","hash":1972518504}
{"level":"info","ts":"2024-02-28T22:50:06.980602Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1972518504,"revision":32581,"compact-revision":32162}
{"level":"info","ts":"2024-02-28T22:55:06.970047Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":32820}
{"level":"info","ts":"2024-02-28T22:55:06.971716Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":32820,"took":"1.005361ms","hash":2481155023}
{"level":"info","ts":"2024-02-28T22:55:06.971936Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2481155023,"revision":32820,"compact-revision":32581}
{"level":"info","ts":"2024-02-28T23:00:06.97688Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33060}
{"level":"info","ts":"2024-02-28T23:00:06.978403Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33060,"took":"813.863¬µs","hash":2378455736}
{"level":"info","ts":"2024-02-28T23:00:06.978465Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2378455736,"revision":33060,"compact-revision":32820}
{"level":"info","ts":"2024-02-28T23:05:06.984828Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33301}
{"level":"info","ts":"2024-02-28T23:05:06.985809Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33301,"took":"629.188¬µs","hash":3608306858}
{"level":"info","ts":"2024-02-28T23:05:06.985871Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3608306858,"revision":33301,"compact-revision":33060}
{"level":"info","ts":"2024-02-28T23:10:06.991543Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33543}
{"level":"info","ts":"2024-02-28T23:10:06.992698Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33543,"took":"760.854¬µs","hash":796242043}
{"level":"info","ts":"2024-02-28T23:10:06.992811Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":796242043,"revision":33543,"compact-revision":33301}
{"level":"info","ts":"2024-02-28T23:15:06.998059Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":33960}
{"level":"info","ts":"2024-02-28T23:15:07.013073Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":33960,"took":"14.371526ms","hash":1423941054}
{"level":"info","ts":"2024-02-28T23:15:07.013219Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1423941054,"revision":33960,"compact-revision":33543}
{"level":"info","ts":"2024-02-28T23:20:07.004261Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34372}
{"level":"info","ts":"2024-02-28T23:20:07.018491Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34372,"took":"13.405337ms","hash":2664385925}
{"level":"info","ts":"2024-02-28T23:20:07.01856Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2664385925,"revision":34372,"compact-revision":33960}
{"level":"info","ts":"2024-02-28T23:25:07.010217Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":34618}
{"level":"info","ts":"2024-02-28T23:25:07.025579Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":34618,"took":"14.67866ms","hash":446027381}
{"level":"info","ts":"2024-02-28T23:25:07.025651Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":446027381,"revision":34618,"compact-revision":34372}

* 
* ==> kernel <==
*  23:26:00 up  1:54,  0 users,  load average: 1.51, 1.59, 1.62
Linux minikube 6.5.0-21-generic #21~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Fri Feb  9 13:32:52 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [96e3bcc8c0ec] <==
* I0228 22:15:08.432221       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0228 22:15:08.432826       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0228 22:15:08.432221       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0228 22:15:08.433108       1 secure_serving.go:213] Serving securely on [::]:8443
I0228 22:15:08.433161       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0228 22:15:08.433799       1 handler_discovery.go:412] Starting ResourceDiscoveryManager
I0228 22:15:08.434837       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0228 22:15:08.434893       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0228 22:15:08.434918       1 aggregator.go:164] waiting for initial CRD sync...
I0228 22:15:08.434928       1 controller.go:78] Starting OpenAPI AggregationController
I0228 22:15:08.435153       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0228 22:15:08.435607       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0228 22:15:08.437982       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0228 22:15:08.438036       1 shared_informer.go:311] Waiting for caches to sync for cluster_authentication_trust_controller
I0228 22:15:08.438075       1 available_controller.go:423] Starting AvailableConditionController
I0228 22:15:08.438080       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0228 22:15:08.438221       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0228 22:15:08.438297       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0228 22:15:08.439379       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0228 22:15:08.439559       1 shared_informer.go:311] Waiting for caches to sync for crd-autoregister
I0228 22:15:08.442634       1 controller.go:116] Starting legacy_token_tracking_controller
I0228 22:15:08.442733       1 shared_informer.go:311] Waiting for caches to sync for configmaps
I0228 22:15:08.442767       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0228 22:15:08.444339       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0228 22:15:08.444594       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0228 22:15:08.444708       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0228 22:15:08.444905       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0228 22:15:08.445411       1 controller.go:134] Starting OpenAPI controller
I0228 22:15:08.445647       1 controller.go:85] Starting OpenAPI V3 controller
I0228 22:15:08.445665       1 naming_controller.go:291] Starting NamingConditionController
I0228 22:15:08.445682       1 establishing_controller.go:76] Starting EstablishingController
I0228 22:15:08.445691       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0228 22:15:08.445700       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0228 22:15:08.445708       1 crd_finalizer.go:266] Starting CRDFinalizer
I0228 22:15:08.558710       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0228 22:15:08.560588       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0228 22:15:08.561589       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0228 22:15:08.561638       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0228 22:15:08.561721       1 shared_informer.go:318] Caches are synced for configmaps
E0228 22:15:08.593280       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0228 22:15:08.638414       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0228 22:15:08.642975       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0228 22:15:08.647559       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0228 22:15:08.647626       1 aggregator.go:166] initial CRD sync complete...
I0228 22:15:08.647634       1 autoregister_controller.go:141] Starting autoregister controller
I0228 22:15:08.647639       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0228 22:15:08.647644       1 cache.go:39] Caches are synced for autoregister controller
I0228 22:15:08.648297       1 shared_informer.go:318] Caches are synced for node_authorizer
I0228 22:15:09.453441       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0228 22:15:10.824241       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0228 22:15:10.847975       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0228 22:15:10.905512       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0228 22:15:10.946081       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0228 22:15:10.956779       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E0228 22:15:18.580021       1 storage.go:475] Address {10.244.0.57  0xc00475f780 0xc003525880} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.77}] vs 10.244.0.57 (kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-tvcrm))
E0228 22:15:18.580118       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.57  0xc00475f780 0xc003525880}] [] [{ 8000 TCP <nil>}]}
E0228 22:15:18.599469       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: d19b368d-f409-4248-9eb6-cc4d129e47b9, UID in object meta: "
I0228 22:15:21.073148       1 controller.go:624] quota admission added evaluator for: endpoints
I0228 22:15:21.120751       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0228 22:21:49.673913       1 controller.go:624] quota admission added evaluator for: replicasets.apps

* 
* ==> kube-apiserver [b2dd5e584e27] <==
* }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.401715       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.401788       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.401852       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.401906       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.407783       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.408401       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0228 07:43:25.470366       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
E0228 07:43:25.723179       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0228 07:43:25.723643       1 wrap.go:54] timeout or abort while handling: method=GET URI="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" audit-ID="98c48f1a-cb2e-40b6-9002-260b603ab10c"
E0228 07:43:25.723667       1 timeout.go:142] post-timeout activity - time-elapsed: 2.134¬µs, GET "/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result: <nil>

* 
* ==> kube-controller-manager [2617c05c93dc] <==
* I0228 07:24:47.618040       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0228 07:24:47.618096       1 shared_informer.go:318] Caches are synced for GC
I0228 07:24:47.633211       1 shared_informer.go:318] Caches are synced for persistent volume
I0228 07:24:47.634930       1 shared_informer.go:318] Caches are synced for deployment
I0228 07:24:47.636560       1 shared_informer.go:318] Caches are synced for endpoint_slice
I0228 07:24:47.639170       1 shared_informer.go:318] Caches are synced for ReplicaSet
I0228 07:24:47.640640       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="32.14¬µs"
I0228 07:24:47.640728       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="28.714¬µs"
I0228 07:24:47.643917       1 shared_informer.go:318] Caches are synced for resource quota
I0228 07:24:47.652607       1 shared_informer.go:318] Caches are synced for job
I0228 07:24:47.652768       1 shared_informer.go:318] Caches are synced for stateful set
I0228 07:24:47.665875       1 shared_informer.go:318] Caches are synced for daemon sets
I0228 07:24:47.670370       1 shared_informer.go:318] Caches are synced for attach detach
I0228 07:24:47.675394       1 shared_informer.go:318] Caches are synced for disruption
I0228 07:24:47.682228       1 shared_informer.go:318] Caches are synced for PVC protection
I0228 07:24:47.682432       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deployment-5954c759fd" duration="42.895917ms"
I0228 07:24:47.694051       1 shared_informer.go:318] Caches are synced for endpoint
I0228 07:24:47.697835       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deployment-5954c759fd" duration="110.797¬µs"
I0228 07:24:47.701715       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="61.34378ms"
I0228 07:24:47.701886       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="68.768¬µs"
I0228 07:24:47.708238       1 shared_informer.go:318] Caches are synced for resource quota
I0228 07:24:48.052729       1 shared_informer.go:318] Caches are synced for garbage collector
I0228 07:24:48.052778       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I0228 07:24:48.089605       1 shared_informer.go:318] Caches are synced for garbage collector
I0228 07:24:49.087082       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deployment-5954c759fd" duration="8.749181ms"
I0228 07:24:49.087187       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deployment-5954c759fd" duration="44.704¬µs"
I0228 07:25:02.524486       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="12.950866ms"
I0228 07:25:02.524751       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="219.36¬µs"
I0228 07:25:07.628565       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="115.326¬µs"
I0228 07:25:10.060348       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="13.777514ms"
I0228 07:25:10.060458       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="50.525¬µs"
I0228 07:25:19.866123       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="10.661414ms"
I0228 07:25:19.866880       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="67.806¬µs"
I0228 07:31:55.300169       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deployment-5954c759fd" duration="8.435¬µs"
I0228 07:38:50.430327       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app-deploy-f948c8595 to 6"
I0228 07:38:50.442124       1 event.go:307] "Event occurred" object="default/app-deploy-f948c8595" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-f948c8595-vpsnl"
I0228 07:38:50.450433       1 event.go:307] "Event occurred" object="default/app-deploy-f948c8595" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-f948c8595-h8mk6"
I0228 07:38:50.456000       1 event.go:307] "Event occurred" object="default/app-deploy-f948c8595" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-f948c8595-g7r67"
I0228 07:38:50.484722       1 event.go:307] "Event occurred" object="default/app-deploy-f948c8595" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-f948c8595-stcrv"
I0228 07:38:50.497824       1 event.go:307] "Event occurred" object="default/app-deploy-f948c8595" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-f948c8595-pc6q8"
I0228 07:38:50.498090       1 event.go:307] "Event occurred" object="default/app-deploy-f948c8595" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-f948c8595-xmzvc"
I0228 07:38:50.512746       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="82.616423ms"
I0228 07:38:50.556927       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="44.089321ms"
I0228 07:38:50.594398       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="37.381197ms"
I0228 07:38:50.594528       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="66.274¬µs"
I0228 07:38:50.594956       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="61.676¬µs"
I0228 07:38:50.611186       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="48.451¬µs"
I0228 07:38:50.624176       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="117.119¬µs"
I0228 07:38:53.937876       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="6.005257ms"
I0228 07:38:53.937962       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="32.772¬µs"
I0228 07:38:55.080229       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="5.120854ms"
I0228 07:38:55.080392       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="129.753¬µs"
I0228 07:38:56.108174       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="6.093083ms"
I0228 07:38:56.108393       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="57.839¬µs"
I0228 07:38:57.139880       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="6.584855ms"
I0228 07:38:57.139993       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="40.686¬µs"
I0228 07:38:58.164878       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="6.429791ms"
I0228 07:38:58.165065       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="59.661¬µs"
I0228 07:39:00.208284       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="6.715979ms"
I0228 07:39:00.208349       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-f948c8595" duration="34.274¬µs"

* 
* ==> kube-controller-manager [f1782f317d4d] <==
* I0228 23:11:40.241649       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="48.301¬µs"
I0228 23:11:40.246117       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="274.324¬µs"
I0228 23:11:40.264852       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="5.810365ms"
I0228 23:11:40.265123       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="106.779¬µs"
I0228 23:11:40.270752       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set app-deploy-7f6445c8ff to 3 from 4"
I0228 23:11:40.275923       1 event.go:307] "Event occurred" object="default/app-deploy-7f6445c8ff" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: app-deploy-7f6445c8ff-zctpc"
I0228 23:11:40.284920       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="15.019719ms"
I0228 23:11:40.289442       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app-deploy-7f7d8f446 to 5 from 4"
I0228 23:11:40.293298       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="8.310602ms"
I0228 23:11:40.293405       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="51.076¬µs"
I0228 23:11:40.294065       1 event.go:307] "Event occurred" object="default/app-deploy-7f7d8f446" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-7f7d8f446-z928z"
I0228 23:11:40.310448       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="22.294052ms"
I0228 23:11:40.320352       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="9.764985ms"
I0228 23:11:40.334674       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="14.25696ms"
I0228 23:11:40.335072       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="115.906¬µs"
I0228 23:11:40.662266       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="144.891¬µs"
I0228 23:11:41.302402       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="55.224¬µs"
I0228 23:11:41.314534       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="56.485¬µs"
I0228 23:11:41.317278       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="48.802¬µs"
I0228 23:11:42.369280       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="5.456211ms"
I0228 23:11:42.369387       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="35.146¬µs"
I0228 23:11:42.374466       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set app-deploy-7f6445c8ff to 2 from 3"
I0228 23:11:42.382601       1 event.go:307] "Event occurred" object="default/app-deploy-7f6445c8ff" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: app-deploy-7f6445c8ff-x7rsx"
I0228 23:11:42.394158       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="19.47747ms"
I0228 23:11:42.394676       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set app-deploy-7f7d8f446 to 6 from 5"
I0228 23:11:42.396604       1 event.go:307] "Event occurred" object="default/app-deploy-7f7d8f446" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: app-deploy-7f7d8f446-gjp7j"
I0228 23:11:42.406107       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="11.897756ms"
I0228 23:11:42.407517       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="17.387311ms"
I0228 23:11:42.408881       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="74.97¬µs"
I0228 23:11:42.422582       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="14.609401ms"
I0228 23:11:42.422731       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="78.958¬µs"
I0228 23:11:42.424252       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="62.006¬µs"
I0228 23:11:42.728701       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="54.252¬µs"
I0228 23:11:43.411835       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="65.492¬µs"
I0228 23:11:43.429504       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="51.867¬µs"
I0228 23:11:43.433720       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="52.038¬µs"
I0228 23:11:43.450597       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="5.783592ms"
I0228 23:11:43.450691       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="38.582¬µs"
I0228 23:11:43.457822       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set app-deploy-7f6445c8ff to 1 from 2"
I0228 23:11:43.465285       1 event.go:307] "Event occurred" object="default/app-deploy-7f6445c8ff" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: app-deploy-7f6445c8ff-7q6dg"
I0228 23:11:43.475212       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="17.851841ms"
I0228 23:11:43.492793       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="17.510674ms"
I0228 23:11:43.492951       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="64.3¬µs"
I0228 23:11:43.822628       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="135.664¬µs"
I0228 23:11:44.470396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="61.926¬µs"
I0228 23:11:44.486847       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="54.452¬µs"
I0228 23:11:44.489447       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="50.264¬µs"
I0228 23:11:45.535593       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="11.11911ms"
I0228 23:11:45.535743       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="81.703¬µs"
I0228 23:11:45.540331       1 event.go:307] "Event occurred" object="default/app-deploy" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set app-deploy-7f6445c8ff to 0 from 1"
I0228 23:11:45.550289       1 event.go:307] "Event occurred" object="default/app-deploy-7f6445c8ff" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: app-deploy-7f6445c8ff-zt7jj"
I0228 23:11:45.566993       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="27.258581ms"
I0228 23:11:45.586563       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="19.486188ms"
I0228 23:11:45.587550       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="79.278¬µs"
I0228 23:11:45.977008       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="58.048¬µs"
I0228 23:11:46.568875       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="52.197¬µs"
I0228 23:11:46.583910       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="55.444¬µs"
I0228 23:11:46.588725       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f6445c8ff" duration="47.639¬µs"
I0228 23:11:46.616686       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="7.861883ms"
I0228 23:11:46.616799       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/app-deploy-7f7d8f446" duration="54.232¬µs"

* 
* ==> kube-proxy [0f649d71d2ec] <==
* I0228 22:15:16.583521       1 server_others.go:69] "Using iptables proxy"
I0228 22:15:16.685028       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0228 22:15:17.068931       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0228 22:15:17.110556       1 server_others.go:152] "Using iptables Proxier"
I0228 22:15:17.110647       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0228 22:15:17.110659       1 server_others.go:438] "Defaulting to no-op detect-local"
I0228 22:15:17.111702       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0228 22:15:17.112264       1 server.go:846] "Version info" version="v1.28.3"
I0228 22:15:17.112281       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0228 22:15:17.116454       1 config.go:188] "Starting service config controller"
I0228 22:15:17.120291       1 config.go:315] "Starting node config controller"
I0228 22:15:17.120437       1 shared_informer.go:311] Waiting for caches to sync for node config
I0228 22:15:17.126068       1 config.go:97] "Starting endpoint slice config controller"
I0228 22:15:17.126222       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0228 22:15:17.131998       1 shared_informer.go:311] Waiting for caches to sync for service config
I0228 22:15:17.221256       1 shared_informer.go:318] Caches are synced for node config
I0228 22:15:17.227856       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0228 22:15:17.239580       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-proxy [df02b3be5633] <==
* I0228 07:24:41.725821       1 server_others.go:69] "Using iptables proxy"
I0228 07:24:41.834965       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0228 07:24:42.056024       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0228 07:24:42.080305       1 server_others.go:152] "Using iptables Proxier"
I0228 07:24:42.080351       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0228 07:24:42.080356       1 server_others.go:438] "Defaulting to no-op detect-local"
I0228 07:24:42.081599       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0228 07:24:42.085658       1 server.go:846] "Version info" version="v1.28.3"
I0228 07:24:42.086925       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0228 07:24:42.100583       1 config.go:97] "Starting endpoint slice config controller"
I0228 07:24:42.103225       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0228 07:24:42.103251       1 config.go:188] "Starting service config controller"
I0228 07:24:42.103581       1 shared_informer.go:311] Waiting for caches to sync for service config
I0228 07:24:42.103833       1 config.go:315] "Starting node config controller"
I0228 07:24:42.103869       1 shared_informer.go:311] Waiting for caches to sync for node config
I0228 07:24:42.203701       1 shared_informer.go:318] Caches are synced for endpoint slice config
I0228 07:24:42.206047       1 shared_informer.go:318] Caches are synced for node config
I0228 07:24:42.206113       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [67d912d7c9e3] <==
* I0228 07:24:32.687360       1 serving.go:348] Generated self-signed cert in-memory
W0228 07:24:34.989360       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0228 07:24:34.989425       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0228 07:24:34.989437       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0228 07:24:34.989445       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0228 07:24:35.033727       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0228 07:24:35.033757       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0228 07:24:35.037095       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0228 07:24:35.037956       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0228 07:24:35.038033       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0228 07:24:35.038041       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0228 07:24:35.139866       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0228 07:43:25.456898       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0228 07:43:25.457020       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
E0228 07:43:25.457192       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kube-scheduler [b287a5ee23e5] <==
* I0228 22:15:05.623928       1 serving.go:348] Generated self-signed cert in-memory
W0228 22:15:08.546605       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0228 22:15:08.546668       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0228 22:15:08.546681       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0228 22:15:08.546690       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0228 22:15:08.598898       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0228 22:15:08.598968       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0228 22:15:08.604098       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0228 22:15:08.605272       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0228 22:15:08.605380       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0228 22:15:08.605420       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0228 22:15:08.705756       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Feb 28 23:11:36 minikube kubelet[1410]: I0228 23:11:36.193867    1410 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-vj4qb\" (UniqueName: \"kubernetes.io/projected/169cf8fd-bcd9-4015-a541-8666a21f7f1f-kube-api-access-vj4qb\") on node \"minikube\" DevicePath \"\""
Feb 28 23:11:37 minikube kubelet[1410]: I0228 23:11:37.116659    1410 scope.go:117] "RemoveContainer" containerID="80fc415c8f54f4f01fab9e7f9ee335e0d61f821332ef3561cbe7eb90b95f1c09"
Feb 28 23:11:37 minikube kubelet[1410]: I0228 23:11:37.544627    1410 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="169cf8fd-bcd9-4015-a541-8666a21f7f1f" path="/var/lib/kubelet/pods/169cf8fd-bcd9-4015-a541-8666a21f7f1f/volumes"
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.209635    1410 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app-deploy-7f7d8f446-bd9f4" podStartSLOduration=2.3427531200000002 podCreationTimestamp="2024-02-28 23:11:35 +0000 UTC" firstStartedPulling="2024-02-28 23:11:36.318505872 +0000 UTC m=+3395.759637325" lastFinishedPulling="2024-02-28 23:11:38.185347915 +0000 UTC m=+3397.626479368" observedRunningTime="2024-02-28 23:11:39.189628005 +0000 UTC m=+3398.630759458" watchObservedRunningTime="2024-02-28 23:11:39.209595163 +0000 UTC m=+3398.650726626"
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.240812    1410 topology_manager.go:215] "Topology Admit Handler" podUID="192815d5-55d6-473f-a31e-d2d5f345fbc9" podNamespace="default" podName="app-deploy-7f7d8f446-jhtms"
Feb 28 23:11:39 minikube kubelet[1410]: E0228 23:11:39.241030    1410 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="169cf8fd-bcd9-4015-a541-8666a21f7f1f" containerName="nginx-container"
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.241113    1410 memory_manager.go:346] "RemoveStaleState removing state" podUID="169cf8fd-bcd9-4015-a541-8666a21f7f1f" containerName="nginx-container"
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.323067    1410 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7npqt\" (UniqueName: \"kubernetes.io/projected/192815d5-55d6-473f-a31e-d2d5f345fbc9-kube-api-access-7npqt\") pod \"app-deploy-7f7d8f446-jhtms\" (UID: \"192815d5-55d6-473f-a31e-d2d5f345fbc9\") " pod="default/app-deploy-7f7d8f446-jhtms"
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.624988    1410 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-5bzk5\" (UniqueName: \"kubernetes.io/projected/361d9ff0-17c2-46d5-a383-74340b706609-kube-api-access-5bzk5\") pod \"361d9ff0-17c2-46d5-a383-74340b706609\" (UID: \"361d9ff0-17c2-46d5-a383-74340b706609\") "
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.628924    1410 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/361d9ff0-17c2-46d5-a383-74340b706609-kube-api-access-5bzk5" (OuterVolumeSpecName: "kube-api-access-5bzk5") pod "361d9ff0-17c2-46d5-a383-74340b706609" (UID: "361d9ff0-17c2-46d5-a383-74340b706609"). InnerVolumeSpecName "kube-api-access-5bzk5". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 28 23:11:39 minikube kubelet[1410]: I0228 23:11:39.725904    1410 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-5bzk5\" (UniqueName: \"kubernetes.io/projected/361d9ff0-17c2-46d5-a383-74340b706609-kube-api-access-5bzk5\") on node \"minikube\" DevicePath \"\""
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.217375    1410 scope.go:117] "RemoveContainer" containerID="3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.243459    1410 scope.go:117] "RemoveContainer" containerID="3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f"
Feb 28 23:11:40 minikube kubelet[1410]: E0228 23:11:40.246350    1410 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f" containerID="3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.246393    1410 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f"} err="failed to get container status \"3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f\": rpc error: code = Unknown desc = Error response from daemon: No such container: 3a8faf02c8be67b654d4df1088c6ecc904eced2f40389a91ac4f8011bdb84e9f"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.275732    1410 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app-deploy-7f7d8f446-4pkj2" podStartSLOduration=1.956663865 podCreationTimestamp="2024-02-28 23:11:35 +0000 UTC" firstStartedPulling="2024-02-28 23:11:36.40612052 +0000 UTC m=+3395.847251973" lastFinishedPulling="2024-02-28 23:11:39.725151681 +0000 UTC m=+3399.166283134" observedRunningTime="2024-02-28 23:11:40.259818163 +0000 UTC m=+3399.700949616" watchObservedRunningTime="2024-02-28 23:11:40.275695026 +0000 UTC m=+3399.716826479"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.301464    1410 topology_manager.go:215] "Topology Admit Handler" podUID="0de5c298-f6e2-4ef3-99f7-68346ed513dc" podNamespace="default" podName="app-deploy-7f7d8f446-z928z"
Feb 28 23:11:40 minikube kubelet[1410]: E0228 23:11:40.301690    1410 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="361d9ff0-17c2-46d5-a383-74340b706609" containerName="nginx-container"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.301743    1410 memory_manager.go:346] "RemoveStaleState removing state" podUID="361d9ff0-17c2-46d5-a383-74340b706609" containerName="nginx-container"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.429197    1410 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-46r9g\" (UniqueName: \"kubernetes.io/projected/0de5c298-f6e2-4ef3-99f7-68346ed513dc-kube-api-access-46r9g\") pod \"app-deploy-7f7d8f446-z928z\" (UID: \"0de5c298-f6e2-4ef3-99f7-68346ed513dc\") " pod="default/app-deploy-7f7d8f446-z928z"
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.731098    1410 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-grrrx\" (UniqueName: \"kubernetes.io/projected/42c3e8a6-4361-4381-9069-dee1bf9c7fbe-kube-api-access-grrrx\") pod \"42c3e8a6-4361-4381-9069-dee1bf9c7fbe\" (UID: \"42c3e8a6-4361-4381-9069-dee1bf9c7fbe\") "
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.734650    1410 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/42c3e8a6-4361-4381-9069-dee1bf9c7fbe-kube-api-access-grrrx" (OuterVolumeSpecName: "kube-api-access-grrrx") pod "42c3e8a6-4361-4381-9069-dee1bf9c7fbe" (UID: "42c3e8a6-4361-4381-9069-dee1bf9c7fbe"). InnerVolumeSpecName "kube-api-access-grrrx". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 28 23:11:40 minikube kubelet[1410]: I0228 23:11:40.832440    1410 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-grrrx\" (UniqueName: \"kubernetes.io/projected/42c3e8a6-4361-4381-9069-dee1bf9c7fbe-kube-api-access-grrrx\") on node \"minikube\" DevicePath \"\""
Feb 28 23:11:41 minikube kubelet[1410]: I0228 23:11:41.288956    1410 scope.go:117] "RemoveContainer" containerID="00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a"
Feb 28 23:11:41 minikube kubelet[1410]: I0228 23:11:41.327031    1410 scope.go:117] "RemoveContainer" containerID="00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a"
Feb 28 23:11:41 minikube kubelet[1410]: E0228 23:11:41.328433    1410 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a" containerID="00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a"
Feb 28 23:11:41 minikube kubelet[1410]: I0228 23:11:41.328499    1410 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a"} err="failed to get container status \"00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a\": rpc error: code = Unknown desc = Error response from daemon: No such container: 00d4598a98f051a2f393242b4f6e7a9dfda1c5db7c05a141a064342e605eef8a"
Feb 28 23:11:41 minikube kubelet[1410]: I0228 23:11:41.544329    1410 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="361d9ff0-17c2-46d5-a383-74340b706609" path="/var/lib/kubelet/pods/361d9ff0-17c2-46d5-a383-74340b706609/volumes"
Feb 28 23:11:41 minikube kubelet[1410]: I0228 23:11:41.544952    1410 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="42c3e8a6-4361-4381-9069-dee1bf9c7fbe" path="/var/lib/kubelet/pods/42c3e8a6-4361-4381-9069-dee1bf9c7fbe/volumes"
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.365572    1410 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app-deploy-7f7d8f446-k4c7r" podStartSLOduration=2.535915303 podCreationTimestamp="2024-02-28 23:11:35 +0000 UTC" firstStartedPulling="2024-02-28 23:11:36.421771548 +0000 UTC m=+3395.862903001" lastFinishedPulling="2024-02-28 23:11:41.251246442 +0000 UTC m=+3400.692377895" observedRunningTime="2024-02-28 23:11:42.365299297 +0000 UTC m=+3401.806430750" watchObservedRunningTime="2024-02-28 23:11:42.365390197 +0000 UTC m=+3401.806521650"
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.407933    1410 topology_manager.go:215] "Topology Admit Handler" podUID="57964e58-5c3d-4e2d-82fc-d28ee59f0fa2" podNamespace="default" podName="app-deploy-7f7d8f446-gjp7j"
Feb 28 23:11:42 minikube kubelet[1410]: E0228 23:11:42.408268    1410 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="42c3e8a6-4361-4381-9069-dee1bf9c7fbe" containerName="nginx-container"
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.408304    1410 memory_manager.go:346] "RemoveStaleState removing state" podUID="42c3e8a6-4361-4381-9069-dee1bf9c7fbe" containerName="nginx-container"
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.544903    1410 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-kmx76\" (UniqueName: \"kubernetes.io/projected/57964e58-5c3d-4e2d-82fc-d28ee59f0fa2-kube-api-access-kmx76\") pod \"app-deploy-7f7d8f446-gjp7j\" (UID: \"57964e58-5c3d-4e2d-82fc-d28ee59f0fa2\") " pod="default/app-deploy-7f7d8f446-gjp7j"
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.849156    1410 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mtc9w\" (UniqueName: \"kubernetes.io/projected/8a52e8a2-2301-4766-a93c-c534ae93a52c-kube-api-access-mtc9w\") pod \"8a52e8a2-2301-4766-a93c-c534ae93a52c\" (UID: \"8a52e8a2-2301-4766-a93c-c534ae93a52c\") "
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.855494    1410 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/8a52e8a2-2301-4766-a93c-c534ae93a52c-kube-api-access-mtc9w" (OuterVolumeSpecName: "kube-api-access-mtc9w") pod "8a52e8a2-2301-4766-a93c-c534ae93a52c" (UID: "8a52e8a2-2301-4766-a93c-c534ae93a52c"). InnerVolumeSpecName "kube-api-access-mtc9w". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 28 23:11:42 minikube kubelet[1410]: I0228 23:11:42.950413    1410 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-mtc9w\" (UniqueName: \"kubernetes.io/projected/8a52e8a2-2301-4766-a93c-c534ae93a52c-kube-api-access-mtc9w\") on node \"minikube\" DevicePath \"\""
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.399147    1410 scope.go:117] "RemoveContainer" containerID="e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679"
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.425928    1410 scope.go:117] "RemoveContainer" containerID="e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679"
Feb 28 23:11:43 minikube kubelet[1410]: E0228 23:11:43.429909    1410 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679" containerID="e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679"
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.430933    1410 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679"} err="failed to get container status \"e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679\": rpc error: code = Unknown desc = Error response from daemon: No such container: e03d8404bb2ae99a3d46c09ebfe50cd1554e2e6814ad18564e0a6157f8f02679"
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.444677    1410 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app-deploy-7f7d8f446-jhtms" podStartSLOduration=1.607633903 podCreationTimestamp="2024-02-28 23:11:39 +0000 UTC" firstStartedPulling="2024-02-28 23:11:39.937178468 +0000 UTC m=+3399.378309921" lastFinishedPulling="2024-02-28 23:11:42.774180347 +0000 UTC m=+3402.215311800" observedRunningTime="2024-02-28 23:11:43.444468939 +0000 UTC m=+3402.885600402" watchObservedRunningTime="2024-02-28 23:11:43.444635782 +0000 UTC m=+3402.885767235"
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.544562    1410 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="8a52e8a2-2301-4766-a93c-c534ae93a52c" path="/var/lib/kubelet/pods/8a52e8a2-2301-4766-a93c-c534ae93a52c/volumes"
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.959674    1410 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-gfp9r\" (UniqueName: \"kubernetes.io/projected/93d5776f-db1b-4350-8ad1-febb6d4a5fde-kube-api-access-gfp9r\") pod \"93d5776f-db1b-4350-8ad1-febb6d4a5fde\" (UID: \"93d5776f-db1b-4350-8ad1-febb6d4a5fde\") "
Feb 28 23:11:43 minikube kubelet[1410]: I0228 23:11:43.965850    1410 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/93d5776f-db1b-4350-8ad1-febb6d4a5fde-kube-api-access-gfp9r" (OuterVolumeSpecName: "kube-api-access-gfp9r") pod "93d5776f-db1b-4350-8ad1-febb6d4a5fde" (UID: "93d5776f-db1b-4350-8ad1-febb6d4a5fde"). InnerVolumeSpecName "kube-api-access-gfp9r". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 28 23:11:44 minikube kubelet[1410]: I0228 23:11:44.060593    1410 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-gfp9r\" (UniqueName: \"kubernetes.io/projected/93d5776f-db1b-4350-8ad1-febb6d4a5fde-kube-api-access-gfp9r\") on node \"minikube\" DevicePath \"\""
Feb 28 23:11:44 minikube kubelet[1410]: I0228 23:11:44.458545    1410 scope.go:117] "RemoveContainer" containerID="675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655"
Feb 28 23:11:44 minikube kubelet[1410]: I0228 23:11:44.491086    1410 scope.go:117] "RemoveContainer" containerID="675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655"
Feb 28 23:11:44 minikube kubelet[1410]: E0228 23:11:44.492573    1410 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: 675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655" containerID="675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655"
Feb 28 23:11:44 minikube kubelet[1410]: I0228 23:11:44.492637    1410 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655"} err="failed to get container status \"675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655\": rpc error: code = Unknown desc = Error response from daemon: No such container: 675f7f4e0a63489e7daf22cbcd3042e7152a66d63a40aad036ac3deb53f5b655"
Feb 28 23:11:45 minikube kubelet[1410]: I0228 23:11:45.551347    1410 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/app-deploy-7f7d8f446-z928z" podStartSLOduration=2.140062205 podCreationTimestamp="2024-02-28 23:11:40 +0000 UTC" firstStartedPulling="2024-02-28 23:11:40.99843592 +0000 UTC m=+3400.439567373" lastFinishedPulling="2024-02-28 23:11:44.40967669 +0000 UTC m=+3403.850808143" observedRunningTime="2024-02-28 23:11:45.524587142 +0000 UTC m=+3404.965718615" watchObservedRunningTime="2024-02-28 23:11:45.551302975 +0000 UTC m=+3404.992434428"
Feb 28 23:11:45 minikube kubelet[1410]: I0228 23:11:45.557471    1410 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="93d5776f-db1b-4350-8ad1-febb6d4a5fde" path="/var/lib/kubelet/pods/93d5776f-db1b-4350-8ad1-febb6d4a5fde/volumes"
Feb 28 23:11:46 minikube kubelet[1410]: I0228 23:11:46.077636    1410 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-f9ml5\" (UniqueName: \"kubernetes.io/projected/e7a94c1b-9965-4368-8757-97bfd56ec08d-kube-api-access-f9ml5\") pod \"e7a94c1b-9965-4368-8757-97bfd56ec08d\" (UID: \"e7a94c1b-9965-4368-8757-97bfd56ec08d\") "
Feb 28 23:11:46 minikube kubelet[1410]: I0228 23:11:46.083301    1410 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/e7a94c1b-9965-4368-8757-97bfd56ec08d-kube-api-access-f9ml5" (OuterVolumeSpecName: "kube-api-access-f9ml5") pod "e7a94c1b-9965-4368-8757-97bfd56ec08d" (UID: "e7a94c1b-9965-4368-8757-97bfd56ec08d"). InnerVolumeSpecName "kube-api-access-f9ml5". PluginName "kubernetes.io/projected", VolumeGidValue ""
Feb 28 23:11:46 minikube kubelet[1410]: I0228 23:11:46.178782    1410 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-f9ml5\" (UniqueName: \"kubernetes.io/projected/e7a94c1b-9965-4368-8757-97bfd56ec08d-kube-api-access-f9ml5\") on node \"minikube\" DevicePath \"\""
Feb 28 23:11:46 minikube kubelet[1410]: I0228 23:11:46.557639    1410 scope.go:117] "RemoveContainer" containerID="f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f"
Feb 28 23:11:46 minikube kubelet[1410]: I0228 23:11:46.603050    1410 scope.go:117] "RemoveContainer" containerID="f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f"
Feb 28 23:11:46 minikube kubelet[1410]: E0228 23:11:46.604796    1410 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f" containerID="f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f"
Feb 28 23:11:46 minikube kubelet[1410]: I0228 23:11:46.604829    1410 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f"} err="failed to get container status \"f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f\": rpc error: code = Unknown desc = Error response from daemon: No such container: f184f441d1cecd565496ed4163ca8662cda097d42cc18b2af4a21031d5b7434f"
Feb 28 23:11:47 minikube kubelet[1410]: I0228 23:11:47.542467    1410 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="e7a94c1b-9965-4368-8757-97bfd56ec08d" path="/var/lib/kubelet/pods/e7a94c1b-9965-4368-8757-97bfd56ec08d/volumes"

* 
* ==> kubernetes-dashboard [1bc7208b0731] <==
* 2024/02/28 07:25:19 Starting overwatch
2024/02/28 07:25:19 Using namespace: kubernetes-dashboard
2024/02/28 07:25:19 Using in-cluster config to connect to apiserver
2024/02/28 07:25:19 Using secret token for csrf signing
2024/02/28 07:25:19 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/02/28 07:25:19 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/02/28 07:25:19 Successful initial request to the apiserver, version: v1.28.3
2024/02/28 07:25:19 Generating JWE encryption key
2024/02/28 07:25:19 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/02/28 07:25:19 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/28 07:25:19 Initializing JWE encryption key from synchronized object
2024/02/28 07:25:19 Creating in-cluster Sidecar client
2024/02/28 07:25:19 Successful request to sidecar
2024/02/28 07:25:19 Serving insecurely on HTTP port: 9090

* 
* ==> kubernetes-dashboard [5f12cdc9f341] <==
* 2024/02/28 22:15:18 Using namespace: kubernetes-dashboard
2024/02/28 22:15:18 Using in-cluster config to connect to apiserver
2024/02/28 22:15:18 Using secret token for csrf signing
2024/02/28 22:15:18 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/02/28 22:15:18 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/02/28 22:15:18 Successful initial request to the apiserver, version: v1.28.3
2024/02/28 22:15:18 Generating JWE encryption key
2024/02/28 22:15:18 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/02/28 22:15:18 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/02/28 22:15:18 Initializing JWE encryption key from synchronized object
2024/02/28 22:15:18 Creating in-cluster Sidecar client
2024/02/28 22:15:18 Serving insecurely on HTTP port: 9090
2024/02/28 22:15:18 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2024/02/28 22:15:48 Successful request to sidecar
2024/02/28 22:15:18 Starting overwatch

* 
* ==> storage-provisioner [0bb15cd0daf4] <==
* I0228 22:15:16.167882       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0228 22:15:37.247111       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

* 
* ==> storage-provisioner [65a85133b9bd] <==
* I0228 22:15:48.702667       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0228 22:15:48.721909       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0228 22:15:48.722906       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0228 22:16:06.129051       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0228 22:16:06.129500       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a47f73dd-6b8f-47da-9dcd-a37801f8c51e!
I0228 22:16:06.129442       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"8c4d0b8c-4c06-4f2c-861a-ac24d2a2fba8", APIVersion:"v1", ResourceVersion:"30813", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a47f73dd-6b8f-47da-9dcd-a37801f8c51e became leader
I0228 22:16:06.232518       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a47f73dd-6b8f-47da-9dcd-a37801f8c51e!

